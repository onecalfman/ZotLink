"""
ZotLink Zotero Integration Module

æ‰©å±•ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§å­¦æœ¯æ•°æ®åº“ï¼š
- arXivï¼ˆæ— éœ€è®¤è¯ï¼‰
- Natureï¼ˆéœ€è¦cookiesï¼‰
- æ›´å¤šæ•°æ®åº“ï¼ˆå¯æ‰©å±•ï¼‰
"""

import requests
import json
import time
import re
import sqlite3
import tempfile
import shutil
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
import asyncio
from datetime import datetime

# å¯¼å…¥æå–å™¨ç®¡ç†å™¨
try:
    from .extractors.extractor_manager import ExtractorManager
    EXTRACTORS_AVAILABLE = True
except ImportError:
    try:
        # å¤‡ç”¨å¯¼å…¥è·¯å¾„
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent))
        from extractors.extractor_manager import ExtractorManager
        EXTRACTORS_AVAILABLE = True
    except ImportError:
        EXTRACTORS_AVAILABLE = False
        logging.warning("Extractor manager not availableï¼Œä»…æ”¯æŒarXiv")

from .utils import AuthorParser, DateParser

logger = logging.getLogger(__name__)


class ZoteroConnector:
    """ZotLinkçš„Zoteroè¿æ¥å™¨ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self):
        """Initialize connector"""
        self.base_url = "http://127.0.0.1:23119"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Content-Type': 'application/json'
        })
        
        # åˆå§‹åŒ–é…ç½®ä¸æ•°æ®åº“è·¯å¾„
        self._zotero_storage_dir: Optional[Path] = None
        self._zotero_db_override: Optional[Path] = None
        self._load_config_overrides()
        self._zotero_db_path = self._find_zotero_database()
        
        # åˆå§‹åŒ–æå–å™¨ç®¡ç†å™¨
        if EXTRACTORS_AVAILABLE:
            self.extractor_manager = ExtractorManager()
            logger.info("Extractor manager initialized successfully")
        else:
            self.extractor_manager = None
            logger.warning("Extractor manager not available")

    def _load_config_overrides(self) -> None:
        """Load Zotero path overrides from env vars and configã€‚
        ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > Claudeé…ç½® > æœ¬åœ°é…ç½®æ–‡ä»¶ > é»˜è®¤æ¢æµ‹
        æ”¯æŒï¼š
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_ROOT æŒ‡å®šZoteroæ ¹ç›®å½•ï¼ˆæ¨èï¼Œè‡ªåŠ¨æ¨å¯¼æ•°æ®åº“å’Œå­˜å‚¨è·¯å¾„ï¼‰
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_DB æŒ‡å®šæ•°æ®åº“å®Œæ•´è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_DIR æŒ‡å®šstorageç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
          - é€šè¿‡MCPç¯å¢ƒå˜é‡ä¼ é€’çš„é…ç½®
          - é…ç½®æ–‡ä»¶ ~/.zotlink/config.json ä¸­çš„ zotero.database_path / zotero.storage_dir
        """
        try:
            # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦è®¾ç½®äº†Zoteroæ ¹ç›®å½•ï¼ˆæ¨èæ–¹å¼ï¼‰
            env_root = os.environ.get('ZOTLINK_ZOTERO_ROOT', '').strip()
            if env_root:
                root_path = Path(os.path.expanduser(env_root))
                if root_path.exists():
                    # è‡ªåŠ¨æ¨å¯¼æ•°æ®åº“å’Œå­˜å‚¨è·¯å¾„
                    candidate_db = root_path / "zotero.sqlite"
                    candidate_storage = root_path / "storage"
                    
                    if candidate_db.exists():
                        self._zotero_db_override = candidate_db
                        logger.info(f"Auto-detected DB path from Zotero root: {candidate_db}")
                    
                    if candidate_storage.exists():
                        self._zotero_storage_dir = candidate_storage
                        logger.info(f"Auto-detected storage dir from Zotero root: {candidate_storage}")
                    
                    if not candidate_db.exists() and not candidate_storage.exists():
                        logger.warning(f"Zotero root does not contain expected database or storage")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_ROOTç›®å½•ä¸å­˜åœ¨: {root_path}")
            
            # 2. ç¯å¢ƒå˜é‡ä¼˜å…ˆï¼ˆå‘åå…¼å®¹ï¼Œä¼šè¦†ç›–æ ¹ç›®å½•æ¨å¯¼çš„ç»“æœï¼‰
            env_db = os.environ.get('ZOTLINK_ZOTERO_DB', '').strip()
            if env_db:
                candidate = Path(os.path.expanduser(env_db))
                if candidate.exists():
                    self._zotero_db_override = candidate
                    logger.info(f"Using env var to override Zotero DB path: {candidate}")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DBè·¯å¾„ä¸å­˜åœ¨: {candidate}")
            
            env_storage = os.environ.get('ZOTLINK_ZOTERO_DIR', '').strip()
            if env_storage:
                storage_path = Path(os.path.expanduser(env_storage))
                if storage_path.exists():
                    self._zotero_storage_dir = storage_path
                    logger.info(f"ğŸ”§ ä½¿ç”¨ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DIRæŒ‡å®šstorageç›®å½•: {storage_path}")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DIRç›®å½•ä¸å­˜åœ¨: {storage_path}")

            # Claudeé…ç½®æ–‡ä»¶ï¼ˆè‹¥æœªé€šè¿‡ç¯å¢ƒå˜é‡è®¾å®šï¼‰
            self._load_claude_config()

            # æœ¬åœ°é…ç½®æ–‡ä»¶ï¼ˆè‹¥å‰é¢æ–¹å¼éƒ½æœªè®¾å®šï¼‰
            config_file = Path.home() / '.zotlink' / 'config.json'
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        cfg = json.load(f)
                    zotero_cfg = cfg.get('zotero', {}) if isinstance(cfg, dict) else {}

                    if not self._zotero_db_override:
                        cfg_db = zotero_cfg.get('database_path', '').strip()
                        if cfg_db:
                            cfg_db_path = Path(os.path.expanduser(cfg_db))
                            if cfg_db_path.exists():
                                self._zotero_db_override = cfg_db_path
                                logger.info(f"Using config to override Zotero DB path: {cfg_db_path}")
                            else:
                                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸­database_pathä¸å­˜åœ¨: {cfg_db_path}")

                    if not self._zotero_storage_dir:
                        cfg_storage = zotero_cfg.get('storage_dir', '').strip()
                        if cfg_storage:
                            cfg_storage_path = Path(os.path.expanduser(cfg_storage))
                            if cfg_storage_path.exists():
                                self._zotero_storage_dir = cfg_storage_path
                                logger.info(f"Using config to specify storage directory: {cfg_storage_path}")
                            else:
                                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸­storage_dirä¸å­˜åœ¨: {cfg_storage_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½Zoteroè·¯å¾„è¦†ç›–è®¾ç½®å¤±è´¥: {e}")

    def _load_claude_config(self) -> None:
        """Load Zotero paths from Claude configã€‚
        æ”¯æŒmacOS/Linuxå’ŒWindowsçš„Claudeé…ç½®è·¯å¾„ã€‚
        """
        try:
            # Claudeé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰
            claude_config_paths = [
                Path.home() / "Library" / "Application Support" / "Claude" / "claude_desktop_config.json",  # macOS
                Path.home() / ".config" / "claude" / "claude_desktop_config.json",                          # Linux
                Path.home() / "AppData" / "Roaming" / "Claude" / "claude_desktop_config.json"              # Windows
            ]
            
            for config_path in claude_config_paths:
                if config_path.exists():
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            claude_config = json.load(f)
                        
                        # æŸ¥æ‰¾zotlinkæœåŠ¡å™¨é…ç½®
                        mcp_servers = claude_config.get('mcpServers', {})
                        zotlink_config = mcp_servers.get('zotlink', {})
                        
                        # Claudeé…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œè®°å½•ä½†ä¸å†è¯»å–éæ ‡å‡†MCPå­—æ®µ
                        # æ¨èä½¿ç”¨envç¯å¢ƒå˜é‡æ–¹å¼é…ç½®Zoteroè·¯å¾„
                        logger.debug(f"Found Claude config file: {config_path}")
                        logger.info("Recommended: use env vars for Zotero paths in MCP config")
                        break
                        
                    except Exception as e:
                        logger.warning(f"Failed to read Claude config {config_path}: {e}")
                        
        except Exception as e:
            logger.warning(f"Failed to load Claude config: {e}")
    
    def _extract_arxiv_metadata(self, arxiv_url: str) -> Dict:
        """Extract detailed paper metadata from arXiv URL"""
        try:
            # Extract arXiv ID
            arxiv_id_match = re.search(r'arxiv\.org/(abs|pdf)/([^/?]+)', arxiv_url)
            if not arxiv_id_match:
                return {"error": "Cannot parse arXiv ID"}
            
            arxiv_id = arxiv_id_match.group(2)
            logger.info(f"Extract arXiv ID: {arxiv_id}")
            
            # Get arXiv abstract page
            abs_url = f"https://arxiv.org/abs/{arxiv_id}"
            response = self.session.get(abs_url, timeout=10)
            
            if response.status_code != 200:
                return {"error": f"Cannot access arXiv page: {response.status_code}"}
            
            html_content = response.text
            
            # æå–è®ºæ–‡ä¿¡æ¯
            metadata = {
                'arxiv_id': arxiv_id,
                'abs_url': abs_url,
                'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            }
            
            # Extract title
            title_match = re.search(r'<meta name="citation_title" content="([^"]+)"', html_content)
            if title_match:
                metadata['title'] = title_match.group(1)
            else:
                # å¤‡é€‰æ–¹å¼
                title_match = re.search(r'<h1[^>]*class="title[^"]*"[^>]*>([^<]+)</h1>', html_content)
                if title_match:
                    metadata['title'] = title_match.group(1).replace('Title:', '').strip()
            
            # Extract authors - improved version
            authors = []
            
            # æ–¹æ³•1: ä½¿ç”¨citation_authorå…ƒæ•°æ®ï¼ˆæœ€å‡†ç¡®ï¼‰
            author_matches = re.findall(r'<meta name="citation_author" content="([^"]+)"', html_content)
            if author_matches:
                authors = author_matches
            else:
                # æ–¹æ³•2: ä»ä½œè€…é“¾æ¥ä¸­æå–
                author_section = re.search(r'<div[^>]*class="[^"]*authors[^"]*"[^>]*>(.*?)</div>', html_content, re.DOTALL)
                if author_section:
                    # æå–æ‰€æœ‰ä½œè€…é“¾æ¥
                    author_links = re.findall(r'<a[^>]*href="/search/\?searchtype=author[^"]*">([^<]+)</a>', author_section.group(1))
                    if author_links:
                        authors = [author.strip() for author in author_links]
            
            # Format author list - ç¡®ä¿æ­£ç¡®çš„å§“åæ ¼å¼
            if authors:
                formatted_authors = []
                for author in authors:
                    # å¦‚æœæ˜¯ "Last, First" æ ¼å¼ï¼Œä¿æŒä¸å˜
                    if ',' in author:
                        formatted_authors.append(author.strip())
                    else:
                        # å¦‚æœæ˜¯ "First Last" æ ¼å¼ï¼Œè½¬æ¢ä¸º "Last, First"
                        parts = author.strip().split()
                        if len(parts) >= 2:
                            last_name = parts[-1]
                            first_names = ' '.join(parts[:-1])
                            formatted_authors.append(f"{last_name}, {first_names}")
                        else:
                            formatted_authors.append(author.strip())
                
                metadata['authors'] = formatted_authors
                metadata['authors_string'] = '; '.join(formatted_authors)  # ä½¿ç”¨åˆ†å·åˆ†éš”ï¼Œæ›´æ ‡å‡†
            else:
                metadata['authors'] = []
                metadata['authors_string'] = ''
            
            # Extract abstract - improved version
            abstract = None
            
            # å…ˆå°è¯•æ‰¾åˆ°æ‘˜è¦åŒºåŸŸ
            abstract_section = re.search(r'<blockquote[^>]*class="abstract[^"]*"[^>]*>(.*?)</blockquote>', html_content, re.DOTALL)
            if abstract_section:
                abstract_html = abstract_section.group(1)
                
                # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                abstract_text = re.sub(r'<[^>]+>', ' ', abstract_html)
                abstract_text = re.sub(r'\s+', ' ', abstract_text).strip()
                
                # ç§»é™¤"Abstract:"æ ‡è¯†ç¬¦
                if abstract_text.startswith('Abstract:'):
                    abstract_text = abstract_text[9:].strip()
                
                # è¿‡æ»¤æ‰arXivLabsç›¸å…³å†…å®¹ï¼ˆé€šå¸¸åœ¨æ‘˜è¦æœ€åï¼‰
                lines = abstract_text.split('.')
                filtered_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not any(keyword in line.lower() for keyword in 
                             ['arxivlabs', 'framework that allows', 'collaborators to develop', 
                              'new arxiv features', 'directly on our website']):
                        filtered_lines.append(line)
                    else:
                        break  # é‡åˆ°arXivLabså†…å®¹å°±åœæ­¢
                
                if filtered_lines:
                    abstract = '. '.join(filtered_lines).strip()
                    if abstract.endswith('.'):
                        abstract = abstract[:-1]  # ç§»é™¤æœ€åå¤šä½™çš„å¥å·
                    abstract = abstract + '.'  # æ·»åŠ ç»“æŸå¥å·
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ‘˜è¦ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•
            if not abstract:
                # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ‘˜è¦æ ‡è®°
                alt_patterns = [
                    r'<div[^>]*class="abstract[^"]*"[^>]*>.*?<p[^>]*>(.*?)</p>',
                    r'<meta[^>]+name="description"[^>]+content="([^"]+)"'
                ]
                
                for pattern in alt_patterns:
                    alt_match = re.search(pattern, html_content, re.DOTALL)
                    if alt_match:
                        abstract_candidate = alt_match.group(1).strip()
                        abstract_candidate = re.sub(r'<[^>]+>', '', abstract_candidate)
                        abstract_candidate = re.sub(r'\s+', ' ', abstract_candidate).strip()
                        
                        if len(abstract_candidate) > 50:
                            abstract = abstract_candidate
                            break
            
            if abstract and len(abstract) > 20:
                metadata['abstract'] = abstract
            
            # Extract date - improved version
            date_match = re.search(r'<meta name="citation_date" content="([^"]+)"', html_content)
            if date_match:
                metadata['date'] = date_match.group(1)
            else:
                # å¤‡é€‰æ–¹æ³•ï¼šä»æäº¤ä¿¡æ¯ä¸­æå–
                date_match = re.search(r'\[Submitted on ([^\]]+)\]', html_content)
                if date_match:
                    date_str = date_match.group(1).strip()
                    # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
                    try:
                        import datetime
                        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
                        for fmt in ['%d %b %Y', '%B %d, %Y', '%Y-%m-%d']:
                            try:
                                parsed_date = datetime.strptime(date_str, fmt)
                                metadata['date'] = parsed_date.strftime('%Y/%m/%d')
                                break
                            except ValueError:
                                continue
                        else:
                            metadata['date'] = date_str
                    except:
                        metadata['date'] = date_str
            
            # Extract comment info (pages, figures, etc.)
            comment = None
            
            # æ–¹å¼1: æ ‡å‡†è¡¨æ ¼æ ¼å¼
            comment_match = re.search(r'<td class="comments">([^<]+)</td>', html_content)
            if comment_match:
                comment = comment_match.group(1).strip()
            
            # æ–¹å¼2: Commentsæ ‡ç­¾åçš„å†…å®¹
            if not comment:
                comment_match = re.search(r'Comments:\s*([^\n<]+)', html_content)
                if comment_match:
                    comment = comment_match.group(1).strip()
            
            # æ–¹å¼3: ç›´æ¥æœç´¢é¡µæ•°å’Œå›¾è¡¨ä¿¡æ¯
            if not comment:
                pages_figures = re.search(r'(\d+\s*pages?,?\s*\d*\s*figures?)', html_content, re.IGNORECASE)
                if pages_figures:
                    comment = pages_figures.group(1).strip()
            
            # æ–¹å¼4: æ›´å®½æ³›çš„é¡µæ•°æœç´¢
            if not comment:
                pages_match = re.search(r'(\d+\s*pages?[^<\n]{0,30})', html_content, re.IGNORECASE)
                if pages_match:
                    comment = pages_match.group(1).strip()
            
            if comment:
                metadata['comment'] = comment
            
            # Extract subject classification
            subjects_matches = re.findall(r'<span class="primary-subject">([^<]+)</span>', html_content)
            if subjects_matches:
                metadata['subjects'] = subjects_matches
            else:
                # å¤‡é€‰æ–¹å¼
                subjects_matches = re.findall(r'class="[^"]*subject-class[^"]*">([^<]+)</span>', html_content)
                if subjects_matches:
                    metadata['subjects'] = subjects_matches
            
            # Extract DOI if available
            doi_match = re.search(r'<meta name="citation_doi" content="([^"]+)"', html_content)
            if doi_match:
                metadata['doi'] = doi_match.group(1)
            
            # Extract journal info if published
            journal_match = re.search(r'<meta name="citation_journal_title" content="([^"]+)"', html_content)
            if journal_match:
                metadata['published_journal'] = journal_match.group(1)
            
            # è®¾ç½®é»˜è®¤å€¼
            metadata.setdefault('title', 'Unknown arXiv Paper')
            metadata.setdefault('authors_string', 'Unknown Authors')
            metadata.setdefault('date', time.strftime('%Y'))
            metadata.setdefault('abstract', '')
            
            logger.info(f"Successfully extracted arXiv metadata: {metadata.get('title', 'Unknown')}")
            return metadata
            
        except Exception as e:
            logger.error(f"Failed to extract arXiv metadata: {e}")
            return {"error": f"Metadata extraction failed: {e}"}
    
    def _enhance_paper_info_for_arxiv(self, paper_info: Dict) -> Dict:
        """Enhance metadata for arXiv papers"""
        url = paper_info.get('url', '')
        
        if 'arxiv.org' in url:
            logger.info("Detected arXiv paper, enhancing metadata......")
            arxiv_metadata = self._extract_arxiv_metadata(url)
            
            if 'error' not in arxiv_metadata:
                # Merge metadata, prefer arXiv extracted info
                enhanced_info = paper_info.copy()
                enhanced_info.update({
                    'title': arxiv_metadata.get('title', paper_info.get('title', '')),
                    'authors': arxiv_metadata.get('authors_string', paper_info.get('authors', '')),
                    'abstract': arxiv_metadata.get('abstract', paper_info.get('abstract', '')),
                    'date': arxiv_metadata.get('date', paper_info.get('date', '')),
                    'journal': 'arXiv',
                    'itemType': 'preprint',
                    'url': arxiv_metadata.get('abs_url', url),
                    'arxiv_id': arxiv_metadata.get('arxiv_id', ''),
                    'pdf_url': arxiv_metadata.get('pdf_url', ''),
                    'comment': arxiv_metadata.get('comment', ''),  # æ·»åŠ commentä¿¡æ¯
                    'subjects': arxiv_metadata.get('subjects', []),  # æ·»åŠ å­¦ç§‘ä¿¡æ¯
                    'doi': arxiv_metadata.get('doi', ''),  # æ·»åŠ DOI
                    'published_journal': arxiv_metadata.get('published_journal', ''),  # æ·»åŠ å‘è¡¨æœŸåˆŠ
                })
                
                logger.info(f"arXiv metadata enhancement complete: {enhanced_info.get('title', 'Unknown')}")
                return enhanced_info
            else:
                logger.warning(f"arXiv metadata enhancement failed: {arxiv_metadata.get('error', 'Unknown')}")
        
        return paper_info

    def _build_paper_info_from_doi(self, doi: str) -> Dict[str, Any]:
        """
        Build paper info from a DOI string.
        Supports arXiv DOIs (10.48550/arXiv.XXX) and published DOIs.
        
        Args:
            doi: DOI string (e.g., '10.48550/arXiv.2301.00001' or '10.1038/nature12345')
            
        Returns:
            Dictionary containing paper metadata, or {'error': ...} on failure
        """
        import re
        
        try:
            # Clean up DOI
            doi = doi.strip()
            # Remove URL prefix if present
            doi = re.sub(r'^https?://doi\.org/', '', doi, flags=re.IGNORECASE)
            # Remove 'doi:' prefix
            doi = re.sub(r'^doi:', '', doi, flags=re.IGNORECASE).strip()
            
            logger.info(f"è§£æDOI: {doi}")
            
            # Check if it's an arXiv DOI
            arxiv_match = re.match(r'10\.48550/arXiv\.([\d]+\.[\d]+)', doi, re.IGNORECASE)
            if arxiv_match:
                arxiv_id = arxiv_match.group(1)
                logger.info(f"æ£€æµ‹åˆ°arXiv DOIï¼ŒarXiv ID: {arxiv_id}")
                
                # Use arXiv API extractor
                if self.extractor_manager:
                    arxiv_extractor = self.extractor_manager.get_extractor_for_url(f"https://arxiv.org/abs/{arxiv_id}")
                    if arxiv_extractor and hasattr(arxiv_extractor, '_query_arxiv_api'):
                        metadata = arxiv_extractor._query_arxiv_api(arxiv_id)
                        if 'error' not in metadata:
                            return {
                                'title': metadata.get('title', f'arXiv:{arxiv_id}'),
                                'authors': metadata.get('authors_string', ''),
                                'abstract': metadata.get('abstract', ''),
                                'date': metadata.get('date', ''),
                                'url': f"https://arxiv.org/abs/{arxiv_id}",
                                'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                                'arxiv_id': arxiv_id,
                                'doi': doi,
                                'itemType': 'preprint',
                                'extractor': 'arXiv'
                            }
                
                # Fallback: direct API query
                from .extractors.arxiv_extractor import ArxivAPIExtractor
                arxiv_extractor = ArxivAPIExtractor()
                metadata = arxiv_extractor._query_arxiv_api(arxiv_id)
                if 'error' not in metadata:
                    return {
                        'title': metadata.get('title', f'arXiv:{arxiv_id}'),
                        'authors': metadata.get('authors_string', ''),
                        'abstract': metadata.get('abstract', ''),
                        'date': metadata.get('date', ''),
                        'url': f"https://arxiv.org/abs/{arxiv_id}",
                        'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                        'arxiv_id': arxiv_id,
                        'doi': doi,
                        'itemType': 'preprint',
                        'extractor': 'arXiv'
                    }
                else:
                    return {'error': f'æ— æ³•è·å–arXivå…ƒæ•°æ®: {metadata.get("error", "æœªçŸ¥é”™è¯¯")}'}
            
            # Handle regular DOIs (crossref/Datacite)
            crossref_url = f"https://api.crossref.org/works/{doi}"
            response = self.session.get(crossref_url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if 'message' in data:
                    msg = data['message']
                    
                    title = ''
                    if 'title' in msg and msg['title']:
                        title = msg['title'][0]
                    
                    authors = []
                    if 'author' in msg:
                        for author in msg['author']:
                            last = author.get('family', '')
                            first = author.get('given', '')
                            if last or first:
                                authors.append(f"{last}, {first}".strip(', '))
                    authors_str = '; '.join(authors)
                    
                    date = ''
                    if 'published-print' in msg:
                        date_parts = msg['published-print'].get('date-parts', [])
                        if date_parts and date_parts[0]:
                            date = '/'.join(str(p) for p in date_parts[0])
                    elif 'published-online' in msg:
                        date_parts = msg['published-online'].get('date-parts', [])
                        if date_parts and date_parts[0]:
                            date = '/'.join(str(p) for p in date_parts[0])
                    
                    journal = ''
                    if 'container-title' in msg and msg['container-title']:
                        journal = msg['container-title'][0]
                    
                    abstract = ''
                    if 'abstract' in msg:
                        # Crossref abstracts are often JATS XML
                        abstract = re.sub(r'<[^>]+>', '', msg['abstract'])
                        abstract = re.sub(r'\s+', ' ', abstract).strip()
                    
                    return {
                        'title': title,
                        'authors': authors_str,
                        'abstract': abstract,
                        'date': date,
                        'url': f"https://doi.org/{doi}",
                        'pdf_url': '',
                        'doi': doi,
                        'itemType': 'journalArticle',
                        'publicationTitle': journal,
                        'extractor': 'Crossref'
                    }
            else:
                # Try semantic scholar as fallback
                ss_url = f"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}?fields=title,authors,year,abstract,url,externalIds"
                ss_response = self.session.get(ss_url, timeout=30)
                if ss_response.status_code == 200:
                    data = ss_response.json()
                    if data.get('title'):
                        authors_str = ''
                        if 'authors' in data:
                            authors = []
                            for author in data.get('authors', []):
                                name = author.get('name', '')
                                if name:
                                    parts = name.split()
                                    if len(parts) >= 2:
                                        authors.append(f"{parts[-1]}, {' '.join(parts[:-1])}")
                                    else:
                                        authors.append(name)
                            authors_str = '; '.join(authors)
                        
                        return {
                            'title': data.get('title', ''),
                            'authors': authors_str,
                            'abstract': data.get('abstract', ''),
                            'date': str(data.get('year', '')) if data.get('year') else '',
                            'url': data.get('url', f"https://doi.org/{doi}"),
                            'pdf_url': '',
                            'doi': doi,
                            'itemType': 'journalArticle',
                            'extractor': 'SemanticScholar'
                        }
            
            return {'error': f'æ— æ³•è§£æDOI: {doi}'}
            
        except Exception as e:
            logger.error(f"DOIè§£æå¤±è´¥: {e}")
            return {'error': f'DOIè§£æå¤±è´¥: {e}'}

    def _find_zotero_database(self) -> Optional[Path]:
        """Find Zotero database, prefer override pathã€‚"""
        # è¦†ç›–ä¼˜å…ˆ
        if self._zotero_db_override and Path(self._zotero_db_override).exists():
            logger.info(f"Found Zotero database(è¦†ç›–): {self._zotero_db_override}")
            return self._zotero_db_override

        # æŒ‰ç³»ç»Ÿé»˜è®¤è·¯å¾„æ¢æµ‹
        possible_paths: List[Path] = []
        platform = os.name  # 'posix' / 'nt'

        # é€šç”¨è·¯å¾„
        possible_paths.append(Path.home() / 'Zotero' / 'zotero.sqlite')

        # macOS
        possible_paths.append(Path.home() / 'Library' / 'Application Support' / 'Zotero' / 'zotero.sqlite')
        profiles_base_mac = Path.home() / 'Library' / 'Application Support' / 'Zotero' / 'Profiles'
        if profiles_base_mac.exists():
            for profile_dir in profiles_base_mac.iterdir():
                if profile_dir.is_dir():
                    possible_paths.append(profile_dir / 'zotero.sqlite')

        # Windowsï¼ˆAPPDATA ä¸‹çš„Profilesï¼‰
        appdata = os.environ.get('APPDATA')
        if appdata:
            profiles_base_win = Path(appdata) / 'Zotero' / 'Zotero' / 'Profiles'
            if profiles_base_win.exists():
                for profile_dir in profiles_base_win.iterdir():
                    if profile_dir.is_dir():
                        possible_paths.append(profile_dir / 'zotero.sqlite')

        # Linux å¸¸è§è·¯å¾„ï¼ˆè‹¥ç”¨æˆ·å°†Zoteroæ”¾åœ¨å®¶ç›®å½•ï¼‰
        possible_paths.append(Path.home() / '.zotero' / 'zotero.sqlite')

        for path in possible_paths:
            try:
                if path.exists():
                    logger.info(f"Found Zotero database: {path}")
                    return path
            except Exception:
                continue
        
        logger.warning("æœªFound Zotero databaseæ–‡ä»¶")
        return None

    def _read_collections_from_db(self) -> List[Dict]:
        """Read collections directly from database"""
        if not self._zotero_db_path or not self._zotero_db_path.exists():
            logger.error("Zoteroæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return []
        
        try:
            # Create temp copy to avoid locking issues
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                shutil.copy2(self._zotero_db_path, temp_file.name)
                temp_db_path = temp_file.name
            
            try:
                conn = sqlite3.connect(temp_db_path)
                cursor = conn.cursor()
                
                # æŸ¥è¯¢é›†åˆä¿¡æ¯
                query = """
                SELECT 
                    c.collectionID,
                    c.collectionName,
                    c.parentCollectionID,
                    c.key
                FROM collections c
                ORDER BY c.collectionName
                """
                
                cursor.execute(query)
                rows = cursor.fetchall()
                
                collections = []
                for row in rows:
                    collection_data = {
                        'id': row[0],
                        'name': row[1],
                        'parentCollection': row[2] if row[2] else None,
                        'key': row[3] if row[3] else f"collection_{row[0]}"
                    }
                    collections.append(collection_data)
                
                conn.close()
                logger.info(f"Successfully read N collections from database")
                return collections
                
            finally:
                # Clean up temp files
                try:
                    Path(temp_db_path).unlink()
                except:
                    pass
                    
        except Exception as e:
            logger.error(f"Failed to read database collections: {e}")
            return []
    
    def is_running(self) -> bool:
        """Check if Zotero is running"""
        try:
            response = self.session.get(f"{self.base_url}/connector/ping", timeout=2)
            return response.status_code == 200
        except Exception as e:
            logger.debug(f"Zotero not running or cannot connect: {e}")
            return False
    
    def get_version(self) -> Optional[str]:
        """Get Zotero version info"""
        try:
            if not self.is_running():
                return None
            
            response = self.session.get(f"{self.base_url}/connector/ping", timeout=5)
            if response.status_code == 200:
                # Zotero pingè¿”å›HTMLï¼Œä¸æ˜¯JSON
                if "Zotero is running" in response.text:
                    return "Zotero Desktop (Unknown version)"
                else:
                    return "unknown"
        except Exception as e:
            logger.debug(f"Failed to get Zotero version: {e}")
            return "unknown"
    
    def get_collections(self) -> List[Dict]:
        """Get all collections
        Try direct DB read first, fallback to API
        """
        try:
            if not self.is_running():
                return []
            
            # First try reading directly from database (new solution!)
            logger.info("Attempting to read collections directly from database...")
            db_collections = self._read_collections_from_db()
            
            if db_collections:
                logger.info(f"Successfully got N collections from database")
                return db_collections
            
            # If DB read fails, fallback to API
            logger.info("Database read failed, trying API...")
            api_endpoints = [
                "/api/users/local/collections",  # Zotero 7 æœ¬åœ°API
                "/connector/collections",        # å¯èƒ½çš„Connector API
                "/api/collections"               # å¦ä¸€ç§å¯èƒ½çš„ç«¯ç‚¹
            ]
            
            for endpoint in api_endpoints:
                try:
                    response = self.session.get(f"{self.base_url}{endpoint}", timeout=5)
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if isinstance(data, list):
                                logger.info(f"Successfully got collections from endpoint: {endpoint}")
                                return data
                            elif isinstance(data, dict) and 'collections' in data:
                                return data['collections']
                        except json.JSONDecodeError:
                            continue
                except Exception as e:
                    logger.debug(f"æµ‹è¯•ç«¯ç‚¹{endpoint}å¤±è´¥: {e}")
                    continue
            
            logger.warning("Cannot get collection list via API or database")
            return []
                
        except Exception as e:
            logger.error(f"Failed to get Zotero collections: {e}")
            return []
    
    def save_item_to_zotero(self, paper_info: Dict, pdf_path: Optional[str] = None, 
                           collection_key: Optional[str] = None) -> Dict:
        """
        Save paper to Zotero
        
        Args:
            paper_info: è®ºæ–‡ä¿¡æ¯å­—å…¸
            pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            collection_key: ç›®æ ‡é›†åˆkeyï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: ä¿å­˜ç»“æœ
        """
        try:
            if not self.is_running():
                return {
                    "success": False,
                    "message": "Zotero is not running, please start the Zotero desktop app"
                }
            
            # ğŸ¯ å…³é”®æ‰©å±•ï¼šä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºå…ƒæ•°æ®
            enhanced_paper_info = self._enhance_paper_metadata(paper_info)
            
            # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹ä¿¡æ¯
            if 'error' in enhanced_paper_info:
                logger.warning(f"âš ï¸ å…ƒæ•°æ®å¢å¼ºå¤±è´¥: {enhanced_paper_info['error']}")
                enhanced_paper_info = paper_info

            # Build Zotero item data
            zotero_item = self._convert_to_zotero_format(enhanced_paper_info)
            
            # Save to Zotero
            result = self._save_via_connector(zotero_item, pdf_path, collection_key)
            
            # æ·»åŠ æ‰©å±•ä¿¡æ¯åˆ°ç»“æœ
            if result["success"]:
                result["database"] = enhanced_paper_info.get('extractor', 'arXiv')
                result["enhanced"] = 'extractor' in enhanced_paper_info
            
            # å¯¹äºarxivè®ºæ–‡ï¼Œåœ¨å…ƒæ•°æ®ä¿å­˜æˆåŠŸåå¤„ç†PDF
            if result["success"] and 'arxiv.org' in enhanced_paper_info.get('url', '') and enhanced_paper_info.get('arxiv_id'):
                logger.info("å…ƒæ•°æ®ä¿å­˜æˆåŠŸï¼Œç°åœ¨å¤„ç†PDF...")
                
                # åœ¨Extraå­—æ®µä¸­æ·»åŠ PDFä¿¡æ¯ï¼Œç”¨æˆ·å¯ä»¥æ‰‹åŠ¨ä¸‹è½½
                pdf_url = f"https://arxiv.org/pdf/{enhanced_paper_info['arxiv_id']}.pdf"
                result["pdf_url"] = pdf_url
                result["pdf_info"] = f"PDFå¯ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½: {pdf_url}"
                result["message"] += f"\nğŸ“¥ PDFé“¾æ¥: {pdf_url}"
                
                logger.info(f"âœ… PDFé“¾æ¥å·²æ·»åŠ åˆ°æ¡ç›®ä¿¡æ¯ä¸­: {pdf_url}")
            
            if result["success"]:
                logger.info(f"æˆåŠŸSave to Zotero: {enhanced_paper_info.get('title', 'Unknown Title')}")
                # FIX: Add correct title info to return result
                result["title"] = enhanced_paper_info.get('title', '')
                result["paper_info"] = enhanced_paper_info
            
            return result
            
        except Exception as e:
            logger.error(f"Save to Zoteroå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"Save to Zoteroå¤±è´¥: {e}"
            }
    
    def _convert_to_zotero_format(self, paper_info: Dict) -> Dict:
        """Convert paper info to Zotero format"""
        
        # Parse authors - use centralized AuthorParser
        authors = []
        
        # Use already formatted creators (Zotero format array)
        if paper_info.get('creators') and isinstance(paper_info['creators'], list):
            authors = paper_info['creators'][:15]
        elif paper_info.get('authors'):
            authors = AuthorParser.parse_authors_to_zotero(paper_info['authors'], max_authors=15)
        
        # Parse date - use centralized DateParser
        date = DateParser.normalize(paper_info.get('date', '') or '')
        
        # Determine item type 
        item_type = paper_info.get('itemType', 'journalArticle')
        if 'arxiv.org' in paper_info.get('url', ''):
            item_type = 'preprint'
        
        # æ„å»ºZoteroé¡¹ç›®
        zotero_item = {
            "itemType": item_type,
            "title": paper_info.get('title', ''),
            "creators": authors,
            "abstractNote": paper_info.get('abstract', ''),
            "publicationTitle": self._get_default_publication_title(paper_info),
            "url": paper_info.get('url', ''),
            "date": date
        }
        
        # ğŸ†• ä¸ºé¢„å°æœ¬æ·»åŠ å®˜æ–¹Zotero Connectorå…¼å®¹çš„å­—æ®µ
        if item_type == 'preprint':
            if 'arxiv.org' in paper_info.get('url', '') and paper_info.get('arxiv_id'):
                # arXivç‰¹æ®Šå¤„ç†
                zotero_item["repository"] = "arXiv"
                zotero_item["archiveID"] = f"arXiv:{paper_info['arxiv_id']}"
                zotero_item["libraryCatalog"] = "arXiv.org"
                
                # ç¾å¼æ—¥æœŸæ—¶é—´æ ¼å¼
                import datetime
                now = datetime.datetime.now()
                month = now.month
                day = now.day
                year = now.year
                hour = now.hour
                minute = now.minute
                second = now.second
                
                if hour == 0:
                    hour_12 = 12
                    am_pm = "AM"
                elif hour < 12:
                    hour_12 = hour
                    am_pm = "AM"
                elif hour == 12:
                    hour_12 = 12
                    am_pm = "PM"
                else:
                    hour_12 = hour - 12
                    am_pm = "PM"
                
                us_format = f"{month}/{day}/{year}, {hour_12}:{minute:02d}:{second:02d} {am_pm}"
                zotero_item["accessDate"] = us_format
            else:
                # ğŸ†• å…¶ä»–é¢„å°æœ¬æœåŠ¡å™¨çš„é€šç”¨å¤„ç†
                if paper_info.get('repository'):
                    zotero_item["repository"] = paper_info['repository']
                
                if paper_info.get('archiveID'):
                    zotero_item["archiveID"] = paper_info['archiveID']
                elif paper_info.get('DOI'):
                    # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„archiveIDï¼Œä½¿ç”¨DOI
                    zotero_item["archiveID"] = paper_info['DOI']
                
                if paper_info.get('libraryCatalog'):
                    zotero_item["libraryCatalog"] = paper_info['libraryCatalog']
                
                # æ ‡å‡†è®¿é—®æ—¥æœŸæ ¼å¼
                if paper_info.get('accessDate'):
                    zotero_item["accessDate"] = paper_info['accessDate']
                else:
                    zotero_item["accessDate"] = time.strftime('%Y-%m-%d')
        else:
            # éé¢„å°æœ¬ä½¿ç”¨æ ‡å‡†æ ¼å¼
            zotero_item["accessDate"] = time.strftime('%Y-%m-%d')
        
        # ğŸš¨ ä¿®å¤ï¼šä¸ºarxivè®ºæ–‡æ·»åŠ PDF URLï¼ˆä¾›_save_via_connectorä½¿ç”¨ï¼‰
        if paper_info.get('arxiv_id') and paper_info.get('pdf_url'):
            zotero_item["pdf_url"] = paper_info['pdf_url']  # å…³é”®ï¼šæ·»åŠ pdf_urlå­—æ®µ
        
        # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹ï¼ˆarXivè·¯å¾„ï¼‰
        if paper_info.get('pdf_content'):
            zotero_item["pdf_content"] = paper_info['pdf_content']
            logger.info(f"âœ… ä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹: {len(paper_info['pdf_content'])} bytes")
        
        # æ·»åŠ arxivç‰¹æ®Šå­—æ®µå’Œå¢å¼ºä¿¡æ¯
        if paper_info.get('arxiv_id'):
            # ğŸ†• ä½¿ç”¨å®˜æ–¹Zotero Connectorå…¼å®¹çš„Extraæ ¼å¼: "arXiv:ID [å­¦ç§‘]"
            arxiv_id = paper_info['arxiv_id']
            extra_parts = [f"arXiv:{arxiv_id}"]
            
            # æ·»åŠ ä¸»è¦å­¦ç§‘åˆ†ç±»çš„ç¼©å†™ (å¦‚ [cs] è¡¨ç¤º Computer Science)
            if paper_info.get('subjects'):
                # æå–ç¬¬ä¸€ä¸ªå­¦ç§‘çš„ç¼©å†™
                first_subject = paper_info['subjects'][0]
                # ä»"Computation and Language (cs.CL)"ä¸­æå–"cs"
                subject_match = re.search(r'\(([^.]+)', first_subject)
                if subject_match:
                    subject_abbr = subject_match.group(1)
                    extra_parts.append(f"[{subject_abbr}]")
            
            # æ„å»ºç®€æ´çš„Extraä¿¡æ¯ (ä¸å®˜æ–¹æ’ä»¶æ ¼å¼ä¸€è‡´)
            zotero_item["extra"] = " ".join(extra_parts)
            
            # æ·»åŠ DOIå­—æ®µï¼ˆå¦‚æœæœ‰ï¼‰
            if paper_info.get('doi'):
                zotero_item["DOI"] = paper_info['doi']
            
            # å¦‚æœå·²å‘è¡¨åˆ°æœŸåˆŠï¼Œæ›´æ–°æœŸåˆŠåç§°
            if paper_info.get('published_journal'):
                zotero_item["publicationTitle"] = paper_info['published_journal']
        else:
            # å¤„ç†å…¶ä»–æ•°æ®åº“çš„å…ƒæ•°æ®
            extra_info = f"ä¸‹è½½æ¥æº: ZotLink\n"
            
            if paper_info.get('extractor'):
                extra_info += f"æ•°æ®åº“: {paper_info['extractor']}\n"
            
            # æ·»åŠ DOIå­—æ®µ
            if paper_info.get('DOI'):
                zotero_item["DOI"] = paper_info['DOI']
                extra_info += f"DOI: {paper_info['DOI']}\n"
            elif paper_info.get('doi'):
                zotero_item["DOI"] = paper_info['doi']
                extra_info += f"DOI: {paper_info['doi']}\n"
            
            if paper_info.get('comment'):
                extra_info += f"Comment: {paper_info['comment']}\n"
            
            if paper_info.get('pdf_url'):
                extra_info += f"PDFé“¾æ¥: {paper_info['pdf_url']}\n"
                zotero_item["pdf_url"] = paper_info['pdf_url']

            # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹ï¼ˆéarXivè·¯å¾„ï¼‰
            if paper_info.get('pdf_content'):
                zotero_item["pdf_content"] = paper_info['pdf_content']
                logger.info(f"âœ… ä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹: {len(paper_info['pdf_content'])} bytes")
            
            zotero_item["extra"] = extra_info
        
        # ğŸ”‘ æ·»åŠ PDFé™„ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper_info.get('pdf_url'):
            zotero_item["attachments"] = [{
                "title": "Full Text PDF",
                "url": paper_info['pdf_url'],
                "mimeType": "application/pdf",
                "snapshot": False  # é“¾æ¥é™„ä»¶ï¼Œä¸ä¸‹è½½å†…å®¹
            }]
        
        # ç§»é™¤ç©ºå€¼
        zotero_item = {k: v for k, v in zotero_item.items() if v}
        
        return zotero_item
    
    def _download_pdf_content(self, pdf_url: str) -> Optional[bytes]:
        """
        å°è¯•Download PDF content
        
        Args:
            pdf_url: PDFé“¾æ¥
            
        Returns:
            PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            import requests
            
            # ğŸ§¬ ç‰¹æ®Šå¤„ç†ï¼šbioRxivä½¿ç”¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½
            if 'biorxiv.org' in pdf_url.lower():
                logger.info("ğŸ§¬ æ£€æµ‹åˆ°bioRxiv - å¯åŠ¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½")
                try:
                    # ä½¿ç”¨äº‹ä»¶å¾ªç¯å…¼å®¹çš„å¼‚æ­¥è°ƒç”¨
                    import asyncio
                    # ä½¿ç”¨åŒ…å†…ç›¸å¯¹å¯¼å…¥ï¼Œé¿å…åœ¨è¿è¡Œç¯å¢ƒä¸­æ‰¾ä¸åˆ°é¡¶çº§æ¨¡å—
                    from .extractors.browser_extractor import BrowserExtractor
                    
                    async def download_biorxiv_mcp():
                        async with BrowserExtractor() as extractor:
                            return await extractor._download_biorxiv_with_mcp(extractor, pdf_url)
                    
                    # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
                    import concurrent.futures
                    import threading
                    
                    def run_in_thread():
                        # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(download_biorxiv_mcp())
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_in_thread)
                        pdf_content = future.result(timeout=120)  # æ”¾å®½åˆ°120ç§’
                    
                    if pdf_content:
                        logger.info(f"âœ… MCPæµè§ˆå™¨ä¸‹è½½bioRxiv PDFæˆåŠŸ: {len(pdf_content):,} bytes")
                        return pdf_content
                    else:
                        logger.warning("âš ï¸ MCPæµè§ˆå™¨ä¸‹è½½bioRxiv PDFå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨åçˆ¬è™«ä¸‹è½½å™¨")
                        # å›é€€ï¼šä½¿ç”¨é€šç”¨åçˆ¬è™«ä¸‹è½½å™¨
                        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥ä¸‹è½½å™¨ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                        try:
                            import concurrent.futures
                            import asyncio
                            
                            def run_fallback_thread():
                                new_loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(new_loop)
                                try:
                                    from .tools.anti_crawler_pdf_downloader import download_anti_crawler_pdf_async
                                    return new_loop.run_until_complete(download_anti_crawler_pdf_async(pdf_url))
                                finally:
                                    new_loop.close()
                            
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(run_fallback_thread)
                                fallback_content = future.result(timeout=120)
                        except Exception:
                            fallback_content = None
                        if fallback_content:
                            logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½å™¨æˆåŠŸè·å–PDF: {len(fallback_content):,} bytes")
                            return fallback_content
                        return None
                        
                except Exception as e:
                    logger.error(f"âŒ MCPæµè§ˆå™¨ä¸‹è½½å¼‚å¸¸: {e}")
                    # å¼‚å¸¸ä¹Ÿå°è¯•å¤‡ç”¨ä¸‹è½½å™¨
                    # å¼‚å¸¸è·¯å¾„åŒæ ·åœ¨çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥ä¸‹è½½å™¨
                    try:
                        import concurrent.futures
                        import asyncio
                        
                        def run_fallback_thread():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                from .tools.anti_crawler_pdf_downloader import download_anti_crawler_pdf_async
                                return new_loop.run_until_complete(download_anti_crawler_pdf_async(pdf_url))
                            finally:
                                new_loop.close()
                        
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_fallback_thread)
                            fallback_content = future.result(timeout=120)
                    except Exception:
                        fallback_content = None
                    if fallback_content:
                        logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½å™¨æˆåŠŸè·å–PDF: {len(fallback_content):,} bytes")
                        return fallback_content
                    return None
            else:
                # å¯¹äºæ™®é€šç½‘ç«™ï¼Œä½¿ç”¨HTTPè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                logger.info("ğŸ“¥ ä½¿ç”¨HTTPè¯·æ±‚ä¸‹è½½PDF")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/pdf,*/*',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive'
                }
                
                # ğŸ¯ v1.3.6: æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œè§£å†³ç½‘ç»œä¸­æ–­å¯¼è‡´çš„ä¸‹è½½å¤±è´¥
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
                        
                        if response.status_code == 200:
                            content = response.content
                            
                            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆPDF
                            if content and content.startswith(b'%PDF'):
                                logger.info(f"âœ… HTTPä¸‹è½½æˆåŠŸ: {len(content):,} bytes")
                                return content
                            else:
                                logger.warning("âš ï¸ ä¸‹è½½çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆPDF")
                                return None
                        else:
                            logger.warning(f"âš ï¸ HTTPä¸‹è½½å¤±è´¥: {response.status_code}")
                            return None
                            
                    except (requests.exceptions.ConnectionError, 
                            requests.exceptions.ChunkedEncodingError,
                            requests.exceptions.Timeout) as e:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s
                            logger.warning(f"âš ï¸ PDFä¸‹è½½ä¸­æ–­: {type(e).__name__}ï¼Œ{wait_time}ç§’åé‡è¯• (ç¬¬{attempt+1}/{max_retries}æ¬¡)")
                            import time
                            time.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"âŒ PDF download failedï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {e}")
                            return None
                    
        except Exception as e:
            logger.error(f"âŒ PDFä¸‹è½½å¼‚å¸¸: {e}")
            return None
    
    def _get_default_publication_title(self, paper_info: Dict) -> str:
        """Smart determine default publication title"""
        
        # ä¼˜å…ˆä½¿ç”¨å·²æå–çš„æœŸåˆŠä¿¡æ¯
        if paper_info.get('journal'):
            return paper_info['journal']
        
        if paper_info.get('publicationTitle'):
            return paper_info['publicationTitle']
        
        if paper_info.get('proceedingsTitle'):
            return paper_info['proceedingsTitle']
        
        # æ ¹æ®URLå’Œæå–å™¨ç±»å‹ç¡®å®šé»˜è®¤å€¼
        url = paper_info.get('url', '')
        extractor = paper_info.get('extractor', '')
        
        # arXivè®ºæ–‡
        if 'arxiv.org' in url:
            return 'arXiv'
        
        # ğŸ†• å…¶ä»–é¢„å°æœ¬æœåŠ¡å™¨
        if 'medrxiv.org' in url:
            return 'medRxiv'
        elif 'biorxiv.org' in url:
            return 'bioRxiv'
        elif 'chemrxiv.org' in url:
            return 'ChemRxiv'
        elif 'psyarxiv.com' in url:
            return 'PsyArXiv'
        elif 'socarxiv.org' in url:
            return 'SocArXiv'
        
        # CVFè®ºæ–‡
        if 'thecvf.com' in url or extractor.upper() == 'CVF':
            # ä»URLæ¨æ–­ä¼šè®®åç§°
            if '/ICCV' in url:
                return 'IEEE International Conference on Computer Vision (ICCV)'
            elif '/CVPR' in url:
                return 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'
            elif '/WACV' in url:
                return 'IEEE Winter Conference on Applications of Computer Vision (WACV)'
            else:
                return 'IEEE Computer Vision Conference'
        
        # Natureè®ºæ–‡
        if 'nature.com' in url or extractor.upper() == 'NATURE':
            return 'Nature'
        
        # æ ¹æ®æ¡ç›®ç±»å‹ç¡®å®šé»˜è®¤å€¼
        item_type = paper_info.get('itemType', '')
        if item_type == 'conferencePaper':
            return 'Conference Proceedings'
        elif item_type == 'preprint':
            return 'Preprint Server'
        
        # æœ€ç»ˆé»˜è®¤å€¼
        return 'Unknown Journal'
    
    def _save_via_connector(self, zotero_item: Dict, pdf_path: Optional[str] = None, 
                           collection_key: Optional[str] = None) -> Dict:
        """Save via Connector API - practical solution"""
        try:
            import time
            import json
            import requests
            
            session_id = f"success-test-{int(time.time() * 1000)}"
            
            # ğŸ¯ Follow official plugin method: generate random ID
            import random
            import string
            
            # ç”Ÿæˆ8ä½éšæœºå­—ç¬¦ä¸²IDï¼ˆæ¨¡ä»¿å®˜æ–¹æ’ä»¶ï¼‰
            random_item_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
            
            clean_item = {
                "itemType": zotero_item.get("itemType", "journalArticle"),
                "title": zotero_item.get("title", ""),
                "url": zotero_item.get("url", ""),
                "id": random_item_id,  # å…³é”®ï¼šæ·»åŠ éšæœºID
                "tags": [],
                "notes": [],
                "seeAlso": [],
                "attachments": []
            }
            
            # æ·»åŠ å®Œæ•´å…ƒæ•°æ® - ç¡®ä¿Commentä¿¡æ¯åœ¨Extraå­—æ®µä¸­
            if zotero_item.get("creators"):
                clean_item["creators"] = zotero_item["creators"]
            if zotero_item.get("abstractNote"):
                clean_item["abstractNote"] = zotero_item["abstractNote"]
            if zotero_item.get("date"):
                clean_item["date"] = zotero_item["date"]
            if zotero_item.get("publicationTitle"):
                clean_item["publicationTitle"] = zotero_item["publicationTitle"]
            if zotero_item.get("DOI"):
                clean_item["DOI"] = zotero_item["DOI"]
            
            # ğŸ¯ å…³é”®ï¼šç¡®ä¿Extraå­—æ®µï¼ˆåŒ…å«Commentï¼‰è¢«æ­£ç¡®ä¿å­˜
            if zotero_item.get("extra"):
                clean_item["extra"] = zotero_item["extra"]
                logger.info(f"âœ… Extraå­—æ®µï¼ˆåŒ…å«Commentï¼‰: {len(clean_item['extra'])} characters")
                # æ˜¾ç¤ºcommenté¢„è§ˆ
                if 'Comment:' in clean_item['extra']:
                    comment_line = [line for line in clean_item['extra'].split('\n') if 'Comment:' in line][0]
                    logger.info(f"ğŸ“ Commenté¢„è§ˆ: {comment_line}")
            
            # ç”Ÿæˆitem_idå’Œheadersï¼ˆéœ€è¦åœ¨PDFå¤„ç†å‰å®šä¹‰ï¼‰
            item_id = f"item_{int(time.time() * 1000)}"
            clean_item["id"] = item_id
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Content-Type': 'application/json',
                'X-Zotero-Version': '5.0.97',
                'X-Zotero-Connector-API-Version': '3'
            }
            
            session = requests.Session()
            session.headers.update(headers)
            
            # ğŸ¯ æœ€ç»ˆç­–ç•¥ï¼šä¸åœ¨saveItemsä¸­åŒ…å«é™„ä»¶ï¼Œç¨åæ‰‹åŠ¨è§¦å‘ä¸‹è½½
            pdf_url = zotero_item.get('pdf_url')
            
            if pdf_url:
                logger.info(f"Found PDF link: {pdf_url}")
                logger.info("Will manually trigger PDF download after save")
            
            # Generate random ID for item
            import random
            import string
            item_id = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(8))
            clean_item["id"] = item_id
            
            # æ·»åŠ é“¾æ¥é™„ä»¶ï¼ˆä¸ä¼šè¢«ä¸‹è½½çš„ï¼‰
            if pdf_url:
                if not clean_item.get("attachments"):
                    clean_item["attachments"] = []
                clean_item["attachments"].append({
                    "title": f"{clean_item.get('repository', 'Online')} Snapshot",
                    "url": clean_item.get('url', pdf_url),
                    "snapshot": False
                })
            
            # Build save payload
            payload = {
                "sessionID": session_id,
                "uri": zotero_item.get("url", ""),
                "items": [clean_item]
            }
            
            # Set target collection
            if collection_key:
                tree_view_id = self._get_collection_tree_view_id(collection_key)
                if tree_view_id:
                    payload["target"] = tree_view_id
                    logger.info(f"ğŸ¯ ä½¿ç”¨treeViewID: {tree_view_id}")
            
            # headerså’Œsessionå·²ç»åœ¨ä¸Šé¢å®šä¹‰äº†
            
            # Save item
            response = session.post(f"{self.base_url}/connector/saveItems", json=payload, timeout=30)
            
            if response.status_code not in [200, 201]:
                return {
                    "success": False,
                    "message": f"ä¿å­˜å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                }
            
            logger.info("Item saved successfully")
            
            # CORRECT: Use saveAttachment API for PDF
            pdf_attachment_success = False
            
            if pdf_url:
                logger.info(f"Found PDF link: {pdf_url}")
                
                # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹
                try:
                    if zotero_item.get('pdf_content'):
                        logger.info("Using browser-pre-downloaded PDF content, skipping HTTP")
                        pdf_content = zotero_item['pdf_content']
                    else:
                        logger.info("ğŸ“¥ å¼€å§‹Download PDF content...")
                        pdf_content = self._download_pdf_content(pdf_url)
                    
                    if pdf_content:
                        # ğŸ” è¯Šæ–­ï¼šæ£€æŸ¥ä¸‹è½½å†…å®¹çš„å®é™…ç±»å‹
                        logger.info(f"ğŸ“Š PDFå†…å®¹å¤§å°: {len(pdf_content)} bytes")
                        
                        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDFï¼ˆå‰å‡ ä¸ªå­—èŠ‚åº”è¯¥æ˜¯%PDFï¼‰
                        if pdf_content[:4] != b'%PDF':
                            logger.error(f"âŒ ä¸‹è½½çš„å†…å®¹ä¸æ˜¯PDFï¼å‰20å­—èŠ‚: {pdf_content[:20]}")
                            logger.warning("âš ï¸ å¯èƒ½ä¸‹è½½äº†HTMLé”™è¯¯é¡µé¢ï¼Œè·³è¿‡PDFä¿å­˜")
                        else:
                            logger.info(f"âœ… ç¡®è®¤æ˜¯PDFæ–‡ä»¶ï¼Œç‰ˆæœ¬æ ‡è¯†: {pdf_content[:8]}")
                        
                        # å‡†å¤‡é™„ä»¶å…ƒæ•°æ®
                        import random
                        import string
                        attachment_id = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(8))
                        
                        attachment_metadata = {
                            "id": attachment_id,
                            "url": pdf_url,
                            "contentType": "application/pdf",
                            "parentItemID": clean_item.get("id", ""),  # ä½¿ç”¨itemçš„ID
                            "title": "Full Text PDF"
                        }
                        
                        # è°ƒç”¨saveAttachment API
                        attachment_headers = {
                            "Content-Type": "application/pdf",
                            "X-Metadata": json.dumps(attachment_metadata)
                        }
                        
                        # ğŸ”§ Windowså…¼å®¹æ€§ï¼šå¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå¯¹å¤§æ–‡ä»¶æ›´å®½å®¹
                        timeout_value = 60 if len(pdf_content) > 500000 else 30
                        logger.info(f"â±ï¸ ä½¿ç”¨è¶…æ—¶æ—¶é—´: {timeout_value}ç§’")
                        
                        attachment_response = session.post(
                            f"{self.base_url}/connector/saveAttachment?sessionID={session_id}",
                            data=pdf_content,
                            headers=attachment_headers,
                            timeout=timeout_value
                        )
                        
                        if attachment_response.status_code in [200, 201]:
                            pdf_attachment_success = True
                            logger.info("âœ… PDFé™„ä»¶ä¿å­˜æˆåŠŸï¼")
                        else:
                            logger.warning(f"âš ï¸ PDFé™„ä»¶ä¿å­˜å¤±è´¥: {attachment_response.status_code}")
                            logger.warning(f"âš ï¸ å®Œæ•´å“åº”å†…å®¹: {attachment_response.text}")
                            logger.warning(f"âš ï¸ å“åº”Headers: {dict(attachment_response.headers)}")
                            
                            # ğŸ” é¢å¤–è¯Šæ–­ä¿¡æ¯
                            logger.info(f"ğŸ” è¯·æ±‚URL: {self.base_url}/connector/saveAttachment?sessionID={session_id}")
                            logger.info(f"ğŸ” è¯·æ±‚Headers: {attachment_headers}")
                            logger.info(f"ğŸ” PDFå¤§å°: {len(pdf_content)} bytes")
                            logger.info(f"ğŸ” PDFå‰8å­—èŠ‚: {pdf_content[:8]}")
                            
                            # ğŸ”§ Windowså…¼å®¹æ€§ï¼šå°è¯•å¤‡ç”¨æ–¹æ³•
                            if attachment_response.status_code == 500:
                                logger.info("ğŸ”„ å°è¯•å¤‡ç”¨PDFä¿å­˜æ–¹æ³•...")
                                try:
                                    # æ–¹æ³•2ï¼šä½¿ç”¨åŸºç¡€çš„æ–‡ä»¶ä¸Šä¼ æ–¹å¼
                                    files = {
                                        'file': ('document.pdf', pdf_content, 'application/pdf')
                                    }
                                    backup_response = session.post(
                                        f"{self.base_url}/connector/saveAttachment?sessionID={session_id}",
                                        files=files,
                                        timeout=30
                                    )
                                    if backup_response.status_code in [200, 201]:
                                        pdf_attachment_success = True
                                        logger.info("âœ… å¤‡ç”¨æ–¹æ³•PDFä¿å­˜æˆåŠŸï¼")
                                    else:
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {backup_response.status_code}")
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•å“åº”: {backup_response.text}")
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•Headers: {dict(backup_response.headers)}")
                                except Exception as backup_e:
                                    logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•å¼‚å¸¸: {backup_e}")
                    else:
                        logger.warning("âš ï¸ PDFå†…å®¹ä¸‹è½½å¤±è´¥")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ PDFå¤„ç†å¼‚å¸¸: {e}")
                

            
            # ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ
            collection_move_success = False
            if collection_key:
                tree_view_id = self._get_collection_tree_view_id(collection_key)
                if tree_view_id:
                    try:
                        update_data = {"sessionID": session_id, "target": tree_view_id}
                        update_response = session.post(f"{self.base_url}/connector/updateSession", json=update_data, timeout=30)
                        if update_response.status_code in [200, 201]:
                            collection_move_success = True
                            logger.info("âœ… æˆåŠŸç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ é›†åˆç§»åŠ¨å¤±è´¥: {e}")
            
            # æ„å»ºç»“æœ
            result = {
                "success": True,
                "message": "è®ºæ–‡å·²æˆåŠŸä¿å­˜" + ("ï¼ŒPDFé™„ä»¶å·²æ·»åŠ " if pdf_attachment_success else ""),
                "details": {
                    "metadata_saved": True,
                    "collection_moved": collection_move_success,
                    "pdf_downloaded": pdf_attachment_success,
                    "pdf_error": None if pdf_attachment_success else "PDFé™„ä»¶ä¿å­˜å¤±è´¥" if pdf_url else None,
                    "pdf_method": "attachment" if pdf_attachment_success else "failed" if pdf_url else "none"
                }
            }
            
            return result
                        
        except Exception as e:
            logger.error(f"âŒ å®ç”¨æ–¹æ¡ˆä¿å­˜å¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "success": False,
                "message": f"ä¿å­˜å¤±è´¥: {e}"
            }
    
    def _download_arxiv_pdf(self, arxiv_id: str, title: str) -> Optional[str]:
        """ä¸‹è½½arxiv PDFåˆ°ä¸´æ—¶ç›®å½•"""
        try:
            import tempfile
            import urllib.request
            from urllib.parse import quote
            
            # åˆ›å»ºä¸´æ—¶ä¸‹è½½ç›®å½•
            temp_dir = Path(tempfile.gettempdir()) / "zotero_pdfs"
            temp_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:50] if len(safe_title) > 50 else safe_title
            pdf_filename = f"{arxiv_id}_{safe_title}.pdf"
            pdf_path = temp_dir / pdf_filename
            
            # ä¸‹è½½PDF
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            logger.info(f"ä¸‹è½½PDF: {pdf_url}")
            
            urllib.request.urlretrieve(pdf_url, pdf_path)
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸‹è½½æˆåŠŸä¸”æ˜¯PDF
            if pdf_path.exists() and pdf_path.stat().st_size > 1024:  # è‡³å°‘1KB
                logger.info(f"PDF download successful: {pdf_path}")
                return str(pdf_path)
            else:
                logger.warning("PDF download failedæˆ–æ–‡ä»¶å¤ªå°")
                return None
                
        except Exception as e:
            logger.error(f"ä¸‹è½½PDFå¤±è´¥: {e}")
            return None
    
    def _attach_pdf_to_item(self, item_key: str, pdf_path: str, title: str) -> bool:
        """å°†PDFé™„åŠ åˆ°Zoteroæ¡ç›®"""
        try:
            if not Path(pdf_path).exists():
                logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                return False
            
            # å‡†å¤‡é™„ä»¶æ•°æ®
            attachment_data = {
                "itemType": "attachment",
                "parentItem": item_key,
                "linkMode": "imported_file",
                "title": f"{title} - PDF",
                "filename": Path(pdf_path).name,
                "path": pdf_path,
                "contentType": "application/pdf"
            }
            
            # å°è¯•ä¸åŒçš„é™„ä»¶ä¸Šä¼ ç«¯ç‚¹
            attachment_endpoints = [
                "/connector/attachments",
                "/connector/saveItems",
                "/attachments"
            ]
            
            for endpoint in attachment_endpoints:
                try:
                    # ä½¿ç”¨multipart/form-dataä¸Šä¼ æ–‡ä»¶
                    import requests
                    files = {
                        'file': (Path(pdf_path).name, open(pdf_path, 'rb'), 'application/pdf')
                    }
                    data = {
                        'data': json.dumps(attachment_data)
                    }
                    
                    response = self.session.post(
                        f"{self.base_url}{endpoint}",
                        files=files,
                        data=data,
                        timeout=60
                    )
                    
                    files['file'][1].close()  # å…³é—­æ–‡ä»¶
                    
                    if response.status_code in [200, 201]:
                        logger.info(f"PDFé™„ä»¶ä¸Šä¼ æˆåŠŸ: {endpoint}")
                        return True
                        
                except Exception as e:
                    logger.debug(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}ä¸Šä¼ é™„ä»¶å¤±è´¥: {e}")
                    continue
            
            logger.warning("æ‰€æœ‰é™„ä»¶ä¸Šä¼ ç«¯ç‚¹éƒ½å¤±è´¥äº†")
            return False
            
        except Exception as e:
            logger.error(f"é™„ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def create_collection(self, name: str, parent_key: Optional[str] = None) -> Dict:
        """åˆ›å»ºæ–°é›†åˆ"""
        try:
            if not self.is_running():
                return {
                    "success": False,
                    "message": "Zotero is not running, please start the Zotero desktop app"
                }
            
            collection_data = {
                "name": name,
                "parentCollection": parent_key if parent_key else False
            }
            
            # å°è¯•ä¸åŒçš„åˆ›å»ºç«¯ç‚¹
            create_endpoints = [
                "/api/users/local/collections",
                "/connector/createCollection", 
                "/api/collections"
            ]
            
            for endpoint in create_endpoints:
                try:
                    response = self.session.post(
                        f"{self.base_url}{endpoint}",
                        json=collection_data,
                        timeout=15
                    )
                    
                    if response.status_code in [200, 201]:
                        try:
                            result = response.json()
                            collection_key = result.get('key', '')
                            logger.info(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}æˆåŠŸåˆ›å»ºé›†åˆ: {name}")
                            
                            return {
                                "success": True,
                                "message": f"æˆåŠŸåˆ›å»ºé›†åˆ: {name}",
                                "collection_key": collection_key,
                                "collection_name": name
                            }
                        except json.JSONDecodeError:
                            # å³ä½¿æ²¡æœ‰JSONå“åº”ï¼Œå¦‚æœçŠ¶æ€ç æ­£ç¡®ä¹Ÿè®¤ä¸ºæˆåŠŸ
                            logger.info(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}åˆ›å»ºé›†åˆæˆåŠŸï¼ˆæ— JSONå“åº”ï¼‰")
                            return {
                                "success": True,
                                "message": f"æˆåŠŸåˆ›å»ºé›†åˆ: {name}",
                                "collection_key": "",
                                "collection_name": name
                            }
                    
                    elif response.status_code == 404:
                        logger.debug(f"åˆ›å»ºç«¯ç‚¹ä¸å­˜åœ¨: {endpoint}")
                        continue
                    else:
                        logger.debug(f"ç«¯ç‚¹{endpoint}è¿”å›çŠ¶æ€ç : {response.status_code}")
                        continue
                        
                except Exception as e:
                    logger.debug(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}åˆ›å»ºå¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥äº†
            return {
                "success": False,
                "message": "æ‰€æœ‰åˆ›å»ºç«¯ç‚¹éƒ½ä¸å¯ç”¨ï¼Œå¯èƒ½éœ€è¦æ›´æ–°çš„Zoteroç‰ˆæœ¬æˆ–æ‰‹åŠ¨åœ¨Zoteroä¸­åˆ›å»ºé›†åˆ"
            }
                
        except Exception as e:
            logger.error(f"åˆ›å»ºZoteroé›†åˆå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åˆ›å»ºé›†åˆå¤±è´¥: {e}"
            }

    def _get_collection_tree_view_id(self, collection_key: str) -> Optional[str]:
        """æ ¹æ®collection keyè·å–treeViewIDæ ¼å¼"""
        try:
            # ä»æ•°æ®åº“ä¸­æŸ¥æ‰¾collection ID
            if not self._zotero_db_path or not self._zotero_db_path.exists():
                return None
                
            import tempfile
            import shutil
            import sqlite3
            
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                shutil.copy2(self._zotero_db_path, temp_file.name)
                temp_db_path = temp_file.name
                
            try:
                conn = sqlite3.connect(temp_db_path)
                cursor = conn.cursor()
                
                # æ ¹æ®keyæŸ¥æ‰¾collectionID
                cursor.execute(
                    'SELECT collectionID FROM collections WHERE key = ?', 
                    (collection_key,)
                )
                
                result = cursor.fetchone()
                if result:
                    collection_id = result[0]
                    tree_view_id = f"C{collection_id}"
                    logger.info(f"ğŸ¯ è½¬æ¢: {collection_key} â†’ {tree_view_id}")
                    return tree_view_id
                else:
                    logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°collection key: {collection_key}")
                    return None
                    
                conn.close()
                
            finally:
                try:
                    Path(temp_db_path).unlink()
                except:
                    pass
                    
        except Exception as e:
            logger.error(f"âŒ è·å–treeViewIDå¤±è´¥: {e}")
            return None
    
    def set_database_cookies(self, database_name: str, cookies: str) -> bool:
        """
        ä¸ºç‰¹å®šæ•°æ®åº“è®¾ç½®cookies
        
        Args:
            database_name: æ•°æ®åº“åç§°ï¼ˆå¦‚"Nature"ï¼‰
            cookies: cookieå­—ç¬¦ä¸²
            
        Returns:
            bool: è®¾ç½®æˆåŠŸè¿”å›True
        """
        if not self.extractor_manager:
            logger.error("âŒ æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨")
            return False
        
        return self.extractor_manager.set_database_cookies(database_name, cookies)
    
    def get_supported_databases(self) -> List[Dict]:
        """è·å–æ”¯æŒçš„æ•°æ®åº“åˆ—è¡¨"""
        if not self.extractor_manager:
            return [{'name': 'arXiv', 'requires_auth': False, 'has_cookies': False}]
        
        databases = self.extractor_manager.get_supported_databases()
        
        # æ·»åŠ å†…ç½®çš„arXivæ”¯æŒ
        databases.insert(0, {
            'name': 'arXiv',
            'requires_auth': False,
            'has_cookies': False,
            'supported_types': ['preprint']
        })
        
        return databases
    
    def test_database_access(self, database_name: str) -> Dict:
        """Test database accessçŠ¶æ€"""
        if database_name.lower() == 'arxiv':
            return {
                'database': 'arXiv',
                'status': 'success',
                'message': 'arXivæ— éœ€è®¤è¯ï¼Œè®¿é—®æ­£å¸¸'
            }
        
        if not self.extractor_manager:
            return {
                'database': database_name,
                'status': 'not_supported',
                'message': 'æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨'
            }
        
        return self.extractor_manager.test_database_access(database_name)
    
    def _quick_validate_pdf_link(self, pdf_url: str) -> bool:
        """å¿«é€ŸéªŒè¯PDFé“¾æ¥æ˜¯å¦å¯ç”¨"""
        
        if not pdf_url:
            return False
        
        try:
            import requests
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/pdf,*/*;q=0.8',
            })
            
            # ä½¿ç”¨HEADè¯·æ±‚å¿«é€Ÿæ£€æŸ¥ï¼Œè¶…æ—¶5ç§’
            response = session.head(pdf_url, timeout=5, allow_redirects=True)
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code == 200:
                # æ£€æŸ¥Content-Typeï¼ˆå…è®¸OSFçš„octet-streamï¼‰
                content_type = response.headers.get('Content-Type', '').lower()
                if 'pdf' in content_type or content_type == 'application/octet-stream':
                    logger.info(f"ğŸ” PDFé“¾æ¥éªŒè¯é€šè¿‡: {response.status_code}, {content_type}")
                    return True
                else:
                    logger.warning(f"âš ï¸ PDFé“¾æ¥Content-Typeå¼‚å¸¸: {content_type}")
                    return False
            elif response.status_code == 403:
                logger.warning(f"âš ï¸ PDFé“¾æ¥è¢«403é˜»æ­¢: {pdf_url[:60]}...")
                return False
            elif response.status_code == 404:
                logger.warning(f"âš ï¸ PDFé“¾æ¥ä¸å­˜åœ¨: {pdf_url[:60]}...")
                return False
            else:
                logger.warning(f"âš ï¸ PDFé“¾æ¥çŠ¶æ€å¼‚å¸¸: {response.status_code}")
                return False
                
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯è¶…æ—¶")
            return False
        except requests.exceptions.ConnectionError:
            logger.warning(f"âš ï¸ PDFé“¾æ¥è¿æ¥å¤±è´¥")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯å¼‚å¸¸: {e}")
            return False

    def _enhance_paper_metadata(self, paper_info: Dict) -> Dict:
        """
        ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºè®ºæ–‡å…ƒæ•°æ®ï¼ˆæ”¯æŒå¼‚æ­¥æµè§ˆå™¨æ¨¡å¼ï¼‰
        
        Args:
            paper_info: åŸºæœ¬è®ºæ–‡ä¿¡æ¯
            
        Returns:
            Dict: å¢å¼ºåçš„è®ºæ–‡ä¿¡æ¯
        """
        url = paper_info.get('url', '')
        
        # ğŸ”§ ä¿®å¤ï¼šç²¾ç¡®æ£€æŸ¥arXivï¼Œé¿å…è¯¯åŒ¹é…SocArXivç­‰
        if re.search(r'(?<!soc)(?<!med)(?<!bio)arxiv\.org', url):
            return self._enhance_paper_info_for_arxiv(paper_info)
        
        # ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¤„ç†å…¶ä»–æ•°æ®åº“
        if self.extractor_manager:
            logger.info("ğŸ”„ ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºå…ƒæ•°æ®...")
            
            # ğŸš€ å…³é”®ä¿®å¤ï¼šæ”¯æŒå¼‚æ­¥æµè§ˆå™¨æ¨¡å¼
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
                try:
                    loop = asyncio.get_running_loop()
                    logger.info("ğŸ”„ åœ¨ç°æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æå–...")
                    # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œéœ€è¦ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
                    enhanced_metadata = self._run_async_extraction(url)
                except RuntimeError:
                    # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                    logger.info("ğŸ”„ åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯è¿è¡Œå¼‚æ­¥æå–...")
                    enhanced_metadata = asyncio.run(self.extractor_manager.extract_metadata(url))
            except Exception as e:
                logger.error(f"âŒ å¼‚æ­¥æå–å¤±è´¥: {e}")
                enhanced_metadata = {'error': f'å¼‚æ­¥æå–å¤±è´¥: {e}'}
            
            if 'error' not in enhanced_metadata:
                # åˆå¹¶åŸå§‹ä¿¡æ¯å’Œå¢å¼ºä¿¡æ¯
                enhanced_info = paper_info.copy()
                enhanced_info.update(enhanced_metadata)
                
                logger.info(f"âœ… å…ƒæ•°æ®å¢å¼ºæˆåŠŸ: {enhanced_info.get('title', 'Unknown')}")
                return enhanced_info
            else:
                logger.warning(f"âš ï¸ æå–å™¨å¤„ç†å¤±è´¥: {enhanced_metadata['error']}")
                
                # ğŸ”§ ç½‘ç»œå¤±è´¥æ—¶çš„æ™ºèƒ½å›é€€ï¼šå°è¯•åŸºäºURLçš„æ¨¡å¼æå–
                logger.info("ğŸ”„ å°è¯•åŸºäºURLçš„ç¦»çº¿æ¨¡å¼æå–...")
                
                try:
                    # è·å–é€šç”¨æå–å™¨è¿›è¡ŒURLæ¨¡å¼æå–
                    for extractor in self.extractor_manager.extractors:
                        if extractor.__class__.__name__ == 'GenericOpenAccessExtractor':
                            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¿™ä¸ªURL
                            if extractor.can_handle(url):
                                logger.info("ğŸ“ æ‰¾åˆ°é€šç”¨æå–å™¨ï¼Œæ‰§è¡ŒURLæ¨¡å¼æå–")
                                
                                # åº”ç”¨URLæ¨¡å¼æå–
                                url_metadata = extractor._extract_from_url_patterns({}, url)
                                
                                # ğŸ”§ æ–°å¢ï¼šå°è¯•PDFé“¾æ¥æ„é€ 
                                if not url_metadata.get('pdf_url'):
                                    url_metadata = extractor._search_pdf_links_in_html("", url, url_metadata)
                                
                                if url_metadata.get('pdf_url'):
                                    logger.info(f"âœ… ç¦»çº¿æ¨¡å¼æå–æˆåŠŸï¼Œæ‰¾åˆ°PDFé“¾æ¥")
                                    
                                    # ğŸ”§ æ–°å¢ï¼šéªŒè¯PDFé“¾æ¥çš„å®é™…å¯ç”¨æ€§
                                    pdf_url = url_metadata.get('pdf_url')
                                    pdf_valid = self._quick_validate_pdf_link(pdf_url)
                                    
                                    if pdf_valid:
                                        logger.info(f"âœ… PDFé“¾æ¥éªŒè¯é€šè¿‡")
                                        
                                        # è¯†åˆ«åŸŸåä¿¡æ¯
                                        domain_info = extractor._identify_domain(url)
                                        
                                        # æ„å»ºåŸºç¡€å…ƒæ•°æ®
                                        fallback_info = paper_info.copy()
                                        fallback_info.update(url_metadata)
                                        fallback_info.update({
                                            'itemType': domain_info['type'],
                                            'source': domain_info['source'],
                                            'extractor': f"Generic-{domain_info['source']}",
                                            'url': url
                                        })
                                        
                                        # å¢å¼ºé¢„å°æœ¬å­—æ®µ
                                        fallback_info = extractor._enhance_preprint_fields(fallback_info, url)
                                        
                                        logger.info(f"ğŸ¯ ç¦»çº¿å›é€€æˆåŠŸ: {fallback_info.get('repository', 'Unknown')} - PDFé“¾æ¥å·²éªŒè¯")
                                        return fallback_info
                                    else:
                                        logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯å¤±è´¥ï¼Œç»§ç»­å›é€€æµç¨‹")
                                        # ç§»é™¤æ— æ•ˆçš„PDFé“¾æ¥ï¼Œé¿å…è¯¯å¯¼ç”¨æˆ·
                                        url_metadata['pdf_url'] = None
                                else:
                                    logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å¼æå–æœªæ‰¾åˆ°PDFé“¾æ¥")
                                break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„æå–å™¨æˆ–æå–å¤±è´¥ï¼Œè¿”å›åŸå§‹é”™è¯¯
                    logger.warning(f"âŒ ç¦»çº¿å›é€€ä¹Ÿå¤±è´¥ï¼Œè¿”å›åŸå§‹é”™è¯¯")
                    return enhanced_metadata
                    
                except Exception as e:
                    logger.error(f"âŒ ç¦»çº¿å›é€€è¿‡ç¨‹å‡ºé”™: {e}")
                    return enhanced_metadata
        
        # å¦‚æœæ²¡æœ‰æå–å™¨ç®¡ç†å™¨ï¼Œè¿”å›åŸå§‹ä¿¡æ¯
        logger.info("â„¹ï¸ ä½¿ç”¨åŸºæœ¬è®ºæ–‡ä¿¡æ¯")
        return paper_info

    def _run_async_extraction(self, url: str) -> Dict:
        """
        åœ¨ç°æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æå–çš„è¾…åŠ©æ–¹æ³•
        """
        import concurrent.futures
        import threading
        import asyncio
        
        try:
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ
            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        self.extractor_manager.extract_metadata(url)
                    )
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result(timeout=180)  # å¢åŠ åˆ°180ç§’è¶…æ—¶ï¼Œç»™æµè§ˆå™¨è¶³å¤Ÿæ—¶é—´
                
        except concurrent.futures.TimeoutError:
            logger.error("âŒ æµè§ˆå™¨æ¨¡å¼è¶…æ—¶ï¼ˆè¶…è¿‡3åˆ†é’Ÿï¼‰")
            return {'error': 'æµè§ˆå™¨æ¨¡å¼è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«æœºåˆ¶å‡çº§'}
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹æ± æ‰§è¡Œå¼‚å¸¸: {e}")
            return {'error': f'çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {e}'}

    def _validate_pdf_content(self, pdf_data: bytes, headers: dict, pdf_url: str) -> dict:
        """
        éªŒè¯ä¸‹è½½çš„PDFå†…å®¹æ˜¯å¦æœ‰æ•ˆå’Œå®Œæ•´
        
        Args:
            pdf_data: PDFæ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®
            headers: HTTPå“åº”å¤´
            pdf_url: PDFä¸‹è½½URL
            
        Returns:
            dict: éªŒè¯ç»“æœ {"is_valid": bool, "reason": str, "details": dict}
        """
        try:
            pdf_size = len(pdf_data)
            content_type = headers.get('Content-Type', '').lower()
            
            logger.info(f"ğŸ” PDFéªŒè¯å¼€å§‹: {pdf_size} bytes, Content-Type: {content_type}")
            
            # æ£€æŸ¥1: åŸºæœ¬å¤§å°éªŒè¯
            if pdf_size < 1024:  # å°äº1KBè‚¯å®šæœ‰é—®é¢˜
                return {
                    "is_valid": False,
                    "reason": f"æ–‡ä»¶å¤ªå° ({pdf_size} bytes)ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢",
                    "details": {"size": pdf_size, "content_preview": pdf_data[:200].decode('utf-8', errors='ignore')[:100]}
                }
            
            # æ£€æŸ¥2: Content-TypeéªŒè¯ (ğŸ”§ ä¿®å¤ï¼šå…è®¸OSFçš„octet-streamæ ¼å¼)
            if content_type and 'pdf' not in content_type:
                # ğŸ”§ ç‰¹æ®Šå¤„ç†ï¼šapplication/octet-streamå¯èƒ½æ˜¯æœ‰æ•ˆPDFï¼ˆå¦‚OSFï¼‰
                if content_type != 'application/octet-stream':
                    return {
                        "is_valid": False,
                        "reason": f"Content-Typeä¸æ˜¯PDF: {content_type}",
                        "details": {"content_type": content_type, "size": pdf_size}
                    }
                else:
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ°octet-streamç±»å‹ï¼Œå°†é€šè¿‡PDFé­”æœ¯å­—èŠ‚éªŒè¯")
            
            # æ£€æŸ¥3: PDFé­”æœ¯å­—èŠ‚
            if not pdf_data.startswith(b'%PDF'):
                return {
                    "is_valid": False,
                    "reason": "æ–‡ä»¶ä¸ä»¥PDFé­”æœ¯å­—èŠ‚å¼€å¤´",
                    "details": {"size": pdf_size, "start_bytes": pdf_data[:20].hex()}
                }
            
            # æ£€æŸ¥4: HTMLå†…å®¹æ£€æµ‹ï¼ˆæœ‰äº›æœåŠ¡å™¨è¿”å›HTMLé¡µé¢ä½†ä¼ªé€ PDFå¤´ï¼‰
            pdf_text = pdf_data[:2048].decode('utf-8', errors='ignore').lower()
            html_indicators = ['<html', '<body', '<div', '<!doctype', '<title>']
            found_html = [indicator for indicator in html_indicators if indicator in pdf_text]
            
            if found_html:
                return {
                    "is_valid": False,
                    "reason": f"æ–‡ä»¶åŒ…å«HTMLå†…å®¹ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢: {found_html}",
                    "details": {"size": pdf_size, "html_indicators": found_html}
                }
            
            # æ£€æŸ¥5: Natureç‰¹å®šçš„å¤§å°éªŒè¯
            if 'nature.com' in pdf_url.lower():
                if pdf_size < 500000:  # Nature PDFé€šå¸¸è‡³å°‘500KB
                    logger.warning(f"âš ï¸ Nature PDFå¤§å°å¼‚å¸¸: {pdf_size} bytes (é€šå¸¸åº”è¯¥>500KB)")
                    return {
                        "is_valid": False,
                        "reason": f"Nature PDFå¤§å°å¼‚å¸¸: {pdf_size/1024:.1f}KB (é€šå¸¸åº”è¯¥>500KB)",
                        "details": {"size": pdf_size, "expected_min_size": 500000, "url": pdf_url}
                    }
            
            # æ£€æŸ¥6: PDFç»“æ„åŸºæœ¬éªŒè¯
            if b'%%EOF' not in pdf_data[-1024:]:  # PDFæ–‡ä»¶åº”è¯¥ä»¥%%EOFç»“å°¾
                logger.warning("âš ï¸ PDFæ–‡ä»¶å¯èƒ½ä¸å®Œæ•´ï¼ˆç¼ºå°‘EOFæ ‡è®°ï¼‰")
                return {
                    "is_valid": False,
                    "reason": "PDFæ–‡ä»¶ä¸å®Œæ•´ï¼ˆç¼ºå°‘ç»“å°¾æ ‡è®°ï¼‰",
                    "details": {"size": pdf_size, "has_eof": False}
                }
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
            logger.info(f"âœ… PDFéªŒè¯é€šè¿‡: {pdf_size} bytes ({pdf_size/1024:.1f}KB)")
            return {
                "is_valid": True,
                "reason": "PDFéªŒè¯é€šè¿‡",
                "details": {
                    "size": pdf_size,
                    "size_kb": round(pdf_size/1024, 1),
                    "content_type": content_type,
                    "has_pdf_header": True,
                    "has_eof": True
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ PDFéªŒè¯è¿‡ç¨‹å¼‚å¸¸: {e}")
            return {
                "is_valid": False,
                "reason": f"PDFéªŒè¯å¼‚å¸¸: {e}",
                "details": {"exception": str(e)}
            }

    def _analyze_pdf_status(self, pdf_success: bool, pdf_attempts: int, pdf_errors: list) -> dict:
        """
        åˆ†æPDFä¸‹è½½å’Œä¿å­˜çŠ¶æ€
        
        Args:
            pdf_success: PDFæ˜¯å¦æˆåŠŸ
            pdf_attempts: PDFå°è¯•æ¬¡æ•°  
            pdf_errors: PDFé”™è¯¯åˆ—è¡¨
            
        Returns:
            dict: PDFçŠ¶æ€åˆ†æç»“æœ
        """
        if pdf_attempts == 0:
            return {
                "status": "none",
                "message": "æœªå‘ç°PDFä¸‹è½½é“¾æ¥",
                "success": False,
                "details": "è®ºæ–‡å¯èƒ½ä¸åŒ…å«å¯ä¸‹è½½çš„PDFï¼Œæˆ–éœ€è¦ç‰¹æ®Šæƒé™"
            }
        
        if pdf_success:
            return {
                "status": "success", 
                "message": "PDFé™„ä»¶ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ",
                "success": True,
                "details": f"æˆåŠŸå¤„ç† {pdf_attempts} ä¸ªPDFé™„ä»¶"
            }
        else:
            error_summary = "; ".join(pdf_errors) if pdf_errors else "æœªçŸ¥é”™è¯¯"
            return {
                "status": "failed",
                "message": "PDF download failed", 
                "success": False,
                "details": error_summary,
                "suggestion": self._get_pdf_error_suggestion(pdf_errors)
            }
    
    def _get_pdf_error_suggestion(self, pdf_errors: list) -> str:
        """æ ¹æ®PDFé”™è¯¯æä¾›è§£å†³å»ºè®®"""
        if not pdf_errors:
            return "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒPDFé“¾æ¥æœ‰æ•ˆæ€§"
        
        error_text = " ".join(pdf_errors).lower()
        
        if "403" in error_text or "è®¤è¯" in error_text:
            return "è¯·åœ¨Claude Desktopä¸­è®¾ç½®æœ‰æ•ˆçš„æ•°æ®åº“è®¤è¯cookies"
        elif "404" in error_text:
            return "PDFé“¾æ¥å¯èƒ½å·²å¤±æ•ˆï¼Œè¯·å°è¯•å…¶ä»–ä¸‹è½½æº"
        elif "html" in error_text:
            return "ä¸‹è½½åˆ°ç™»å½•é¡µé¢ï¼Œéœ€è¦æ›´æ–°è®¤è¯ä¿¡æ¯"
        else:
            return "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¨åé‡è¯•"
    
    def _generate_save_message(self, pdf_status: dict, collection_moved: bool) -> str:
        """
        ç”Ÿæˆä¿å­˜ç»“æœçš„ç”¨æˆ·å‹å¥½æ¶ˆæ¯
        
        Args:
            pdf_status: PDFçŠ¶æ€ä¿¡æ¯
            collection_moved: æ˜¯å¦ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ
            
        Returns:
            str: ç”¨æˆ·æ¶ˆæ¯
        """
        base_msg = "âœ… è®ºæ–‡åŸºæœ¬ä¿¡æ¯å·²Save to Zotero"
        
        if pdf_status.get("success", False):
            base_msg += "\nâœ… PDFé™„ä»¶ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ"
        elif pdf_status.get("status") == "none":
            base_msg += "\nâ„¹ï¸ æœªå‘ç°å¯ä¸‹è½½çš„PDFé“¾æ¥"
        else:
            base_msg += f"\nâš ï¸ PDF download failed: {pdf_status.get('details', 'æœªçŸ¥åŸå› ')}"
            if pdf_status.get("suggestion"):
                base_msg += f"\nğŸ’¡ å»ºè®®: {pdf_status['suggestion']}"
        
        if collection_moved:
            base_msg += "\nâœ… å·²ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ"
        
        return base_msg
    
    def load_cookies_from_files(self) -> Dict[str, bool]:
        """
        ä»æ–‡ä»¶åŠ è½½æ‰€æœ‰å¯ç”¨çš„cookies
        æ”¯æŒå¤šç§æ ¼å¼å’Œä½ç½®ï¼š
        1. ~/.zotlink/cookies.json (æ¨èä½ç½®ï¼Œå¤šæ•°æ®åº“)
        2. é¡¹ç›®æ ¹ç›®å½•/cookies.json (å‘åå…¼å®¹)
        3. shared_cookies_*.json (ä¹¦ç­¾åŒæ­¥)
        4. ~/.zotlink/nature_cookies.txt (å‘åå…¼å®¹)
        
        Returns:
            Dict[str, bool]: æ¯ä¸ªæ•°æ®åº“çš„åŠ è½½çŠ¶æ€
        """
        import os
        from pathlib import Path
        import time
        from datetime import datetime, timezone
        
        results = {}
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        # ç¡®ä¿ç”¨æˆ·é…ç½®ç›®å½•å­˜åœ¨
        user_config_dir.mkdir(exist_ok=True)
        
        logger.info("ğŸ” æ­£åœ¨æ‰«æcookieæ–‡ä»¶...")
        
        # 1. ä¼˜å…ˆåŠ è½½cookies.jsonï¼ˆä¸»é…ç½®æ–‡ä»¶ï¼‰- ä¼˜å…ˆä»ç”¨æˆ·é…ç½®ç›®å½•åŠ è½½
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        if json_config_file:
            logger.info(f"ğŸ“ æ‰¾åˆ°ä¸»Cookieé…ç½®æ–‡ä»¶: {json_config_file}")
            try:
                with open(json_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                databases = config.get('databases', {})
                loaded_count = 0
                
                for db_key, db_config in databases.items():
                    if db_config.get('status') == 'active' and db_config.get('cookies'):
                        cookies_str = db_config['cookies']
                        cookie_count = db_config.get('cookie_count', len(cookies_str.split(';')))
                        db_name = db_config.get('name', db_key)
                        
                        # è®¾ç½®åˆ°å¯¹åº”æ•°æ®åº“
                        success = self.set_database_cookies(db_key, cookies_str)
                        if success:
                            logger.info(f"âœ… ä»JSONåŠ è½½ {db_name} cookiesæˆåŠŸï¼š{cookie_count}ä¸ªcookies")
                            loaded_count += 1
                        else:
                            logger.error(f"âŒ è®¾ç½® {db_name} cookieså¤±è´¥")
                        results[db_key] = success
                    else:
                        logger.info(f"â¸ï¸ {db_config.get('name', db_key)}: æœªæ¿€æ´»æˆ–æ— cookies")
                        results[db_key] = False
                
                if loaded_count > 0:
                    logger.info(f"ğŸ¯ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ•°æ®åº“çš„cookies")
                    return results
                else:
                    logger.warning("âš ï¸ cookies.jsonä¸­æ²¡æœ‰æ¿€æ´»çš„æ•°æ®åº“cookies")
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–cookies.jsonå¤±è´¥ï¼š{e}")
                results['json_config'] = False
        
        # 2. å…¼å®¹æ€§æ”¯æŒï¼šæ£€æŸ¥nature_cookies.txtæ–‡ä»¶ - ä¼˜å…ˆä»ç”¨æˆ·é…ç½®ç›®å½•åŠ è½½
        txt_cookie_paths = [
            user_config_dir / "nature_cookies.txt",  # æ¨èä½ç½®
            project_root / "nature_cookies.txt"      # å‘åå…¼å®¹
        ]
        
        txt_cookie_file = None
        for path in txt_cookie_paths:
            if path.exists():
                txt_cookie_file = path
                break
                
        if txt_cookie_file:
            logger.info(f"ğŸ“ æ‰¾åˆ°å…¼å®¹æ€§TXTæ–‡ä»¶: {txt_cookie_file}")
            try:
                with open(txt_cookie_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                # è¿‡æ»¤æ³¨é‡Šå’Œç©ºè¡Œ
                lines = [line.strip() for line in content.split('\n') 
                        if line.strip() and not line.strip().startswith('#')]
                
                if lines:
                    cookies_str = ' '.join(lines).strip()
                    cookie_count = len(cookies_str.split(';'))
                    
                    # è®¾ç½®åˆ°Natureæ•°æ®åº“
                    success = self.set_database_cookies('nature', cookies_str)
                    if success:
                        logger.info(f"âœ… ä»TXTæ–‡ä»¶åŠ è½½Nature cookiesæˆåŠŸï¼š{cookie_count}ä¸ªcookies")
                        logger.warning("ğŸ’¡ å»ºè®®è¿ç§»åˆ°cookies.jsonæ ¼å¼ä»¥æ”¯æŒå¤šæ•°æ®åº“")
                    else:
                        logger.error("âŒ è®¾ç½®Nature TXT cookieså¤±è´¥")
                    results['nature_txt'] = success
                else:
                    logger.warning("âš ï¸ nature_cookies.txtæ–‡ä»¶ä¸ºç©ºæˆ–åªåŒ…å«æ³¨é‡Š")
                    results['nature_txt'] = False
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–nature_cookies.txtå¤±è´¥ï¼š{e}")
                results['nature_txt'] = False
        
        # 3. æŸ¥æ‰¾æ‰€æœ‰shared_cookies_*.jsonæ–‡ä»¶ï¼ˆä¹¦ç­¾åŒæ­¥æ ¼å¼ï¼‰
        cookie_files = list(project_root.glob("shared_cookies_*.json"))
        if not cookie_files:
            if not results:
                logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•cookieæ–‡ä»¶")
            return results
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(cookie_files)} ä¸ªcookieæ–‡ä»¶")
        
        for file_path in cookie_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    cookie_data = json.load(f)
                
                site_name = cookie_data.get('siteName', 'Unknown')
                cookies = cookie_data.get('cookies', '')
                timestamp = cookie_data.get('timestamp', '')
                cookies_count = cookie_data.get('cookies_count', 0)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                last_updated = cookie_data.get('last_updated', 0)
                if time.time() - last_updated > 24 * 3600:
                    logger.warning(f"âš ï¸ {site_name} cookieså·²è¿‡æœŸï¼ˆ{timestamp}ï¼‰")
                    results[site_name] = False
                    continue
                
                # æ ¹æ®ç«™ç‚¹åæ˜ å°„åˆ°æ•°æ®åº“å
                database_name = self._map_site_to_database(site_name)
                if database_name:
                    success = self.set_database_cookies(database_name, cookies)
                    if success:
                        logger.info(f"âœ… ä»æ–‡ä»¶åŠ è½½ {site_name} cookiesæˆåŠŸï¼š{cookies_count}ä¸ªcookiesï¼ˆ{timestamp}ï¼‰")
                    else:
                        logger.error(f"âŒ è®¾ç½® {site_name} cookieså¤±è´¥")
                    results[site_name] = success
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥ç«™ç‚¹ï¼š{site_name}")
                    results[site_name] = False
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–cookieæ–‡ä»¶ {file_path} å¤±è´¥ï¼š{e}")
                results[file_path.stem] = False
        
        return results
    
    def _map_site_to_database(self, site_name: str) -> str:
        """å°†ç«™ç‚¹åæ˜ å°„åˆ°æ•°æ®åº“å"""
        mapping = {
            'www.nature.com': 'nature',
            'nature.com': 'nature', 
            'www.science.org': 'science',
            'science.org': 'science',
            'ieeexplore.ieee.org': 'ieee',
            'link.springer.com': 'springer'
        }
        return mapping.get(site_name.lower(), '')
    
    def update_database_cookies(self, database_key: str, cookies_str: str) -> bool:
        """
        æ›´æ–°æŒ‡å®šæ•°æ®åº“çš„cookiesåˆ°cookies.jsonæ–‡ä»¶
        
        Args:
            database_key: æ•°æ®åº“æ ‡è¯† (å¦‚ 'nature', 'science')
            cookies_str: Cookieå­—ç¬¦ä¸²
            
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        import json
        from pathlib import Path
        from datetime import datetime, timezone
        
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨èä½ç½®åˆ›å»ºæ–°æ–‡ä»¶
        if not json_config_file:
            user_config_dir.mkdir(exist_ok=True)
            json_config_file = user_config_dir / "cookies.json"
        
        try:
            # è¯»å–ç°æœ‰é…ç½®
            if json_config_file.exists():
                with open(json_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            else:
                logger.error("âŒ cookies.jsonæ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if database_key not in config.get('databases', {}):
                logger.error(f"âŒ æœªçŸ¥æ•°æ®åº“: {database_key}")
                return False
            
            # æ›´æ–°Cookieä¿¡æ¯
            current_time = datetime.now(timezone.utc).isoformat()
            cookie_count = len(cookies_str.split(';')) if cookies_str else 0
            
            config['last_updated'] = current_time
            config['databases'][database_key].update({
                'cookies': cookies_str,
                'last_updated': current_time,
                'cookie_count': cookie_count,
                'status': 'active' if cookies_str else 'inactive'
            })
            
            # ä¿å­˜æ›´æ–°çš„é…ç½®
            with open(json_config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            # åŒæ—¶è®¾ç½®åˆ°ExtractorManager
            success = self.set_database_cookies(database_key, cookies_str)
            
            db_name = config['databases'][database_key].get('name', database_key)
            if success:
                logger.info(f"Updated cookies successfullyï¼š{cookie_count}ä¸ªcookies")
            else:
                logger.error(f"âŒ è®¾ç½® {db_name} cookieså¤±è´¥")
                
            return success
            
        except Exception as e:
            logger.error(f"Failed to update cookiesï¼š{e}")
            return False
    
    def get_databases_status(self) -> Dict[str, Dict]:
        """
        Get status info for all databases
        
        Returns:
            Dict[str, Dict]: æ•°æ®åº“çŠ¶æ€ä¿¡æ¯
        """
        import json
        from pathlib import Path
        
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨èä½ç½®åˆ›å»ºæ–°æ–‡ä»¶
        if not json_config_file:
            user_config_dir.mkdir(exist_ok=True)
            json_config_file = user_config_dir / "cookies.json"
        
        try:
            if not json_config_file.exists():
                return {}
                
            with open(json_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            databases = config.get('databases', {})
            status_info = {}
            
            for db_key, db_config in databases.items():
                status_info[db_key] = {
                    'name': db_config.get('name', db_key),
                    'status': db_config.get('status', 'inactive'),
                    'cookie_count': db_config.get('cookie_count', 0),
                    'last_updated': db_config.get('last_updated'),
                    'domains': db_config.get('domains', []),
                    'description': db_config.get('description', ''),
                    'login_url': db_config.get('login_url', ''),
                    'test_url': db_config.get('test_url', '')
                }
            
            return status_info
            
        except Exception as e:
            logger.error(f"Failed to read database status: {e}")
            return {}

    def get_library_items(self, limit: int = 50, offset: int = 0, include_details: bool = False) -> Dict:
        """
        Get items from the Zotero library.

        Args:
            limit: Maximum number of items to return
            offset: Offset for pagination
            include_details: Whether to include attachments, notes, and tags count

        Returns:
            Dict containing items and metadata
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            items = self._get_items_from_database(limit, offset, include_details)
            return {"success": True, "items": items}

        except Exception as e:
            logger.error(f"Failed to get library items: {e}")
            return {"success": False, "error": str(e)}

    def _get_items_from_database(self, limit: int = 50, offset: int = 0, include_details: bool = False) -> List[Dict]:
        """Get items directly from the Zotero SQLite database"""
        items = []
        
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return items
        
        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT i.itemID, i.key, i.itemTypeID, i.dateAdded, i.dateModified,
                       t.typeName
                FROM items i
                JOIN itemTypes t ON i.itemTypeID = t.itemTypeID
                WHERE i.libraryID = 1 AND i.itemTypeID NOT IN (2, 14)
                ORDER BY i.dateAdded DESC
                LIMIT ? OFFSET ?
            """, (limit, offset))
            
            for row in cursor:
                item = {
                    "itemKey": row["key"],
                    "itemID": row["itemID"],
                    "itemType": row["typeName"],
                    "dateAdded": row["dateAdded"],
                    "dateModified": row["dateModified"]
                }
                
                title_cursor = conn.cursor()
                title_cursor.execute("""
                    SELECT v.value FROM itemData d
                    JOIN fields f ON d.fieldID = f.fieldID
                    JOIN itemDataValues v ON d.valueID = v.valueID
                    WHERE d.itemID = ? AND f.fieldName = 'title'
                """, (row["itemID"],))
                title_row = title_cursor.fetchone()
                if title_row:
                    item["title"] = title_row["value"]
                else:
                    item["title"] = "Untitled"
                
                if include_details:
                    item["attachment_count"] = len(self._get_item_attachments(row["itemID"]))
                    item["note_count"] = len(self._get_item_notes(row["itemID"]))
                    raw_tags = self._get_item_tags(row["itemID"])
                    item["tag_count"] = len(raw_tags)
                    item["tags"] = [t["name"] for t in raw_tags[:5]]
                
                items.append(item)
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Database query failed: {e}")
        
        return items
        
        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT i.itemID, i.key, i.itemTypeID, i.dateAdded, i.dateModified,
                       t.typeName
                FROM items i
                JOIN itemTypes t ON i.itemTypeID = t.itemTypeID
                WHERE i.libraryID = 1
                ORDER BY i.dateAdded DESC
                LIMIT ? OFFSET ?
            """, (limit, offset))
            
            for row in cursor:
                item = {
                    "itemKey": row["key"],
                    "itemID": row["itemID"],
                    "itemType": row["typeName"],
                    "dateAdded": row["dateAdded"],
                    "dateModified": row["dateModified"]
                }
                
                # Get title
                cursor.execute("""
                    SELECT v.value FROM itemData d
                    JOIN fields f ON d.fieldID = f.fieldID
                    JOIN itemDataValues v ON d.valueID = v.valueID
                    WHERE d.itemID = ? AND f.fieldName = 'title'
                """, (row["itemID"],))
                title_row = cursor.fetchone()
                if title_row:
                    item["title"] = title_row["value"]
                else:
                    item["title"] = "Untitled"
                
                items.append(item)
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Database query failed: {e}")
        
        return items

    def search_items(self, query: str) -> Dict:
        """
        Search for items in the Zotero library.

        Args:
            query: Search query string

        Returns:
            Dict containing matching items
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            items = self._search_items_in_database(query)
            return {"success": True, "items": items}

        except Exception as e:
            logger.error(f"Failed to search items: {e}")
            return {"success": False, "error": str(e)}

    def _search_items_in_database(self, query: str) -> List[Dict]:
        """Search items in the Zotero SQLite database"""
        items = []
        
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return items
        
        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT DISTINCT i.itemID, i.key, i.itemTypeID, i.dateAdded, i.dateModified,
                       t.typeName
                FROM items i
                JOIN itemTypes t ON i.itemTypeID = t.itemTypeID
                JOIN itemData d ON i.itemID = d.itemID
                JOIN itemDataValues v ON d.valueID = v.valueID
                JOIN fields f ON d.fieldID = f.fieldID
                WHERE i.libraryID = 1
                  AND (f.fieldName = 'title' AND v.value LIKE ?)
                ORDER BY i.dateAdded DESC
                LIMIT 50
            """, (f"%{query}%",))
            
            for row in cursor:
                item = {
                    "itemKey": row["key"],
                    "itemID": row["itemID"],
                    "itemType": row["typeName"],
                    "dateAdded": row["dateAdded"],
                    "dateModified": row["dateModified"]
                }
                
                cursor.execute("""
                    SELECT v.value FROM itemData d
                    JOIN fields f ON d.fieldID = f.fieldID
                    JOIN itemDataValues v ON d.valueID = v.valueID
                    WHERE d.itemID = ? AND f.fieldName = 'title'
                """, (row["itemID"],))
                title_row = cursor.fetchone()
                if title_row:
                    item["title"] = title_row["value"]
                else:
                    item["title"] = "Untitled"
                
                items.append(item)
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Database search failed: {e}")
        
        return items

    def get_item(self, item_key: str, include_attachments: bool = True) -> Dict:
        """
        Get a specific item by its key.

        Args:
            item_key: The Zotero item key
            include_attachments: Whether to include attachments, notes, and tags (default: True)

        Returns:
            Dict containing item data
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            item = self._get_item_from_database(item_key)
            if not item:
                return {"success": False, "error": "Item not found"}

            if include_attachments:
                item_id = item.get("itemID")
                item["attachments"] = self._get_item_attachments(item_id)
                item["notes"] = self._get_item_notes(item_id)
                raw_tags = self._get_item_tags(item_id)
                item["tags"] = [t["name"] for t in raw_tags]
                item["tags_detail"] = raw_tags

            return {"success": True, "item": item}

        except Exception as e:
            logger.error(f"Failed to get item: {e}")
            return {"success": False, "error": str(e)}

    def _get_item_from_database(self, item_key: str) -> Optional[Dict]:
        """Get a single item from the Zotero SQLite database"""
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return None
        
        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT i.itemID, i.key, i.itemTypeID, i.dateAdded, i.dateModified,
                       t.typeName
                FROM items i
                JOIN itemTypes t ON i.itemTypeID = t.itemTypeID
                WHERE i.key = ? AND i.libraryID = 1
            """, (item_key,))
            
            row = cursor.fetchone()
            if not row:
                return None
            
            item = {
                "itemKey": row["key"],
                "itemID": row["itemID"],
                "itemType": row["typeName"],
                "dateAdded": row["dateAdded"],
                "dateModified": row["dateModified"]
            }
            
            # Get all item data fields
            cursor.execute("""
                SELECT f.fieldName, v.value FROM itemData d
                JOIN fields f ON d.fieldID = f.fieldID
                JOIN itemDataValues v ON d.valueID = v.valueID
                WHERE d.itemID = ?
            """, (row["itemID"],))
            
            for field_row in cursor:
                item[field_row["fieldName"]] = field_row["value"]
            
            # Get creators
            cursor.execute("""
                SELECT c.firstName, c.lastName, ct.creatorType
                FROM creators c
                JOIN itemCreators ic ON c.creatorID = ic.creatorID
                JOIN creatorTypes ct ON ic.creatorTypeID = ct.creatorTypeID
                WHERE ic.itemID = ?
            """, (row["itemID"],))
            
            creators = []
            for creator_row in cursor:
                creators.append({
                    "firstName": creator_row["firstName"] or "",
                    "lastName": creator_row["lastName"] or "",
                    "creatorType": creator_row["creatorType"]
                })
            item["creators"] = creators
            
            conn.close()
            return item
            
        except Exception as e:
            logger.error(f"Database query failed: {e}")
            return None

    def _get_zotero_db_path(self) -> Optional[Path]:
        """Get the Zotero database path"""
        return self._zotero_db_path

    def update_item(self, item_key: str, updates: Dict) -> Dict:
        """
        Update an existing Zotero item's metadata.

        Args:
            item_key: The Zotero item key to update
            updates: Dictionary of fields to update (title, abstract, date, etc.)

        Returns:
            Dict containing the update result
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            db_path = self._get_zotero_db_path()
            if not db_path or not db_path.exists():
                return {"success": False, "error": "Zotero database not found"}

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Get item ID
            cursor.execute("SELECT itemID FROM items WHERE key = ? AND libraryID = 1", (item_key,))
            row = cursor.fetchone()
            if not row:
                conn.close()
                return {"success": False, "error": "Item not found"}

            item_id = row[0]

            # Map field names to field IDs
            field_name_to_id = {
                "title": "title",
                "abstractNote": "abstractNote",
                "date": "date",
                "url": "url",
                "publicationTitle": "publicationTitle",
                "DOI": "DOI",
            }

            updated_fields = []
            for field_name, field_value in updates.items():
                if field_name in field_name_to_id:
                    db_field_name = field_name_to_id[field_name]
                    
                    # Get field ID
                    cursor.execute("SELECT fieldID FROM fields WHERE fieldName = ?", (db_field_name,))
                    field_row = cursor.fetchone()
                    if not field_row:
                        continue
                    field_id = field_row[0]
                    
                    # Check if value exists, if not add it
                    cursor.execute("SELECT valueID FROM itemData WHERE itemID = ? AND fieldID = ?", (item_id, field_id))
                    value_row = cursor.fetchone()
                    
                    if value_row:
                        # Update existing value
                        cursor.execute("""
                            UPDATE itemDataValues SET value = ? 
                            WHERE valueID = (SELECT valueID FROM itemData WHERE itemID = ? AND fieldID = ?)
                        """, (field_value, item_id, field_id))
                    else:
                        # Get max valueID
                        cursor.execute("SELECT MAX(valueID) FROM itemDataValues")
                        max_value_id = cursor.fetchone()[0] or 0
                        
                        # Insert new value
                        cursor.execute("INSERT INTO itemDataValues (valueID, value) VALUES (?, ?)", 
                                       (max_value_id + 1, field_value))
                        
                        # Insert item data reference
                        cursor.execute("INSERT INTO itemData (itemID, fieldID, valueID) VALUES (?, ?, ?)",
                                       (item_id, field_id, max_value_id + 1))
                    
                    updated_fields.append(field_name)

            conn.commit()
            conn.close()

            if updated_fields:
                logger.info(f"Successfully updated item {item_key}: {updated_fields}")
                return {"success": True, "message": f"Item updated: {', '.join(updated_fields)}", "item_key": item_key}
            else:
                return {"success": False, "error": "No fields were updated"}

        except Exception as e:
            logger.error(f"Failed to update item: {e}")
            return {"success": False, "error": str(e)}

    def update_item_tags(self, item_key: str, tags: List[str]) -> Dict:
        """
        Update the tags on an existing item.

        Args:
            item_key: The Zotero item key
            tags: List of tag strings to set

        Returns:
            Dict containing the update result
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            db_path = self._get_zotero_db_path()
            if not db_path or not db_path.exists():
                return {"success": False, "error": "Zotero database not found"}

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Get item ID
            cursor.execute("SELECT itemID FROM items WHERE key = ? AND libraryID = 1", (item_key,))
            row = cursor.fetchone()
            if not row:
                conn.close()
                return {"success": False, "error": "Item not found"}

            item_id = row[0]

            # Delete existing tags
            cursor.execute("DELETE FROM itemTags WHERE itemID = ?", (item_id,))

            # Add new tags
            for tag in tags:
                # Get or create tag
                cursor.execute("SELECT tagID FROM tags WHERE name = ?", (tag,))
                tag_row = cursor.fetchone()
                
                if tag_row:
                    tag_id = tag_row[0]
                else:
                    cursor.execute("SELECT MAX(tagID) FROM tags")
                    max_tag_id = cursor.fetchone()[0] or 0
                    cursor.execute("INSERT INTO tags (tagID, name) VALUES (?, ?)", (max_tag_id + 1, tag))
                    tag_id = max_tag_id + 1
                
                # Link tag to item (type=0 for manual tags)
                cursor.execute("INSERT INTO itemTags (itemID, tagID, type) VALUES (?, ?, 0)", (item_id, tag_id))

            conn.commit()
            conn.close()

            logger.info(f"Successfully updated tags for item: {item_key}")
            return {"success": True, "message": f"Tags updated: {', '.join(tags)}", "item_key": item_key, "tags": tags}

        except Exception as e:
            logger.error(f"Failed to update item tags: {e}")
            return {"success": False, "error": str(e)}

    def delete_item(self, item_key: str) -> Dict:
        """
        Delete an item from the Zotero library.

        Args:
            item_key: The Zotero item key to delete

        Returns:
            Dict containing the deletion result
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            db_path = self._get_zotero_db_path()
            if not db_path or not db_path.exists():
                return {"success": False, "error": "Zotero database not found"}

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Get item ID
            cursor.execute("SELECT itemID FROM items WHERE key = ? AND libraryID = 1", (item_key,))
            row = cursor.fetchone()
            if not row:
                conn.close()
                return {"success": False, "error": "Item not found"}

            item_id = row[0]

            # Delete in correct order (respect foreign keys)
            cursor.execute("DELETE FROM itemTags WHERE itemID = ?", (item_id,))
            cursor.execute("DELETE FROM itemCreators WHERE itemID = ?", (item_id,))
            cursor.execute("DELETE FROM itemData WHERE itemID = ?", (item_id,))
            cursor.execute("DELETE FROM items WHERE itemID = ?", (item_id,))

            conn.commit()
            conn.close()

            logger.info(f"Successfully deleted item: {item_key}")
            return {"success": True, "message": "Item deleted successfully", "item_key": item_key}

        except Exception as e:
            logger.error(f"Failed to delete item: {e}")
            return {"success": False, "error": str(e)}

    def move_item_to_collection(self, item_key: str, collection_key: str) -> Dict:
        """
        Move an item to a different collection.

        Args:
            item_key: The Zotero item key
            collection_key: The target collection key

        Returns:
            Dict containing the move result
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            db_path = self._get_zotero_db_path()
            if not db_path or not db_path.exists():
                return {"success": False, "error": "Zotero database not found"}

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Get item ID
            cursor.execute("SELECT itemID FROM items WHERE key = ? AND libraryID = 1", (item_key,))
            item_row = cursor.fetchone()
            if not item_row:
                conn.close()
                return {"success": False, "error": "Item not found"}

            item_id = item_row[0]

            # Get collection ID
            cursor.execute("SELECT collectionID FROM collections WHERE libraryID = 1 AND collectionKey = ?", (collection_key,))
            coll_row = cursor.fetchone()
            if not coll_row:
                conn.close()
                return {"success": False, "error": "Collection not found"}

            collection_id = coll_row[0]

            # Check if item is already in collection
            cursor.execute("SELECT * FROM collectionItems WHERE collectionID = ? AND itemID = ?", (collection_id, item_id))
            if cursor.fetchone():
                conn.close()
                return {"success": True, "message": "Item already in collection", "item_key": item_key}

            # Add item to collection
            cursor.execute("INSERT INTO collectionItems (collectionID, itemID) VALUES (?, ?)", (collection_id, item_id))

            conn.commit()
            conn.close()

            logger.info(f"Successfully moved item {item_key} to collection {collection_key}")
            return {"success": True, "message": "Item moved successfully", "item_key": item_key}

        except Exception as e:
            logger.error(f"Failed to move item: {e}")
            return {"success": False, "error": str(e)}

    def validate_item_with_arxiv(self, item_key: str) -> Dict:
        """
        Validate a Zotero item against arXiv API data.

        If the item has a DOI, queries arXiv API and compares metadata
        with the current Zotero entry, displaying any differences.

        Args:
            item_key: The Zotero item key to validate

        Returns:
            Dict containing validation results and differences
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            result = self.get_item(item_key)
            if not result.get("success"):
                return {"success": False, "error": f"Could not get item: {result.get('error')}"}

            item_data = result.get("item", {})
            if isinstance(item_data, str):
                try:
                    item_data = json.loads(item_data)
                except json.JSONDecodeError:
                    pass

            zotero_metadata = {
                "title": item_data.get("title", ""),
                "abstract": item_data.get("abstractNote", ""),
                "date": item_data.get("date", ""),
                "url": item_data.get("url", ""),
                "doi": item_data.get("DOI", ""),
                "creators": item_data.get("creators", []),
                "item_type": item_data.get("itemType", ""),
            }

            doi = zotero_metadata.get("doi", "")
            if not doi:
                return {
                    "success": False,
                    "error": "No DOI found in this item",
                    "item_key": item_key,
                    "zotero_metadata": zotero_metadata,
                    "has_doi": False
                }

            arxiv_url = self._get_arxiv_url_from_doi(doi)
            if not arxiv_url:
                return {
                    "success": False,
                    "error": "Could not find arXiv URL from DOI",
                    "item_key": item_key,
                    "doi": doi
                }

            from .extractors.arxiv_extractor import ArxivAPIExtractor
            arxiv_extractor = ArxivAPIExtractor()
            arxiv_metadata = arxiv_extractor.extract_metadata(arxiv_url)

            if "error" in arxiv_metadata:
                return {
                    "success": False,
                    "error": f"arXiv API query failed: {arxiv_metadata['error']}",
                    "item_key": item_key,
                    "doi": doi,
                    "arxiv_url": arxiv_url
                }

            differences = self._compare_metadata(zotero_metadata, arxiv_metadata)

            return {
                "success": True,
                "item_key": item_key,
                "doi": doi,
                "arxiv_url": arxiv_url,
                "zotero_metadata": zotero_metadata,
                "arxiv_metadata": arxiv_metadata,
                "differences": differences,
                "is_match": len(differences) == 0
            }

        except Exception as e:
            logger.error(f"Failed to validate item: {e}")
            return {"success": False, "error": str(e)}

    def _get_arxiv_url_from_doi(self, doi: str) -> Optional[str]:
        """Extract arXiv URL from DOI if it points to arXiv"""
        if not doi:
            return None

        doi_lower = doi.lower()

        if "arxiv.org" in doi_lower:
            arxiv_id_match = re.search(r'(\d+\.\d+)', doi)
            if arxiv_id_match:
                return f"https://arxiv.org/abs/{arxiv_id_match.group(1)}"

        if "10.48550/arxiv" in doi_lower or "arxiv." in doi_lower:
            arxiv_id_match = re.search(r'arxiv[\.:]*(\d+\.\d+)', doi, re.IGNORECASE)
            if arxiv_id_match:
                return f"https://arxiv.org/abs/{arxiv_id_match.group(1)}"

        return None

    def _compare_metadata(self, zotero: Dict, arxiv: Dict) -> Dict[str, List[Dict]]:
        """Compare Zotero metadata with arXiv metadata and find differences"""
        differences = {}

        title_zotero = (zotero.get("title") or "").strip()
        title_arxiv = (arxiv.get("title") or "").strip()
        if title_zotero.lower() != title_arxiv.lower():
            differences["title"] = [
                {"source": "Zotero", "value": title_zotero},
                {"source": "arXiv", "value": title_arxiv}
            ]

        abstract_zotero = self._normalize_abstract(zotero.get("abstract", ""))
        abstract_arxiv = self._normalize_abstract(arxiv.get("abstract", ""))
        if abstract_zotero != abstract_arxiv:
            differences["abstract"] = [
                {"source": "Zotero", "value": abstract_zotero[:200] + "..." if len(abstract_zotero) > 200 else abstract_zotero},
                {"source": "arXiv", "value": abstract_arxiv[:200] + "..." if len(abstract_arxiv) > 200 else abstract_arxiv}
            ]

        date_zotero = self._normalize_date(zotero.get("date", ""))
        date_arxiv = self._normalize_date(arxiv.get("date", ""))
        if date_zotero and date_arxiv and date_zotero != date_arxiv:
            differences["date"] = [
                {"source": "Zotero", "value": date_zotero},
                {"source": "arXiv", "value": date_arxiv}
            ]

        zotero_authors = self._extract_last_names(zotero.get("creators", []))
        arxiv_authors = [a.get("lastName", "") for a in arxiv.get("authors", [])]
        if zotero_authors and arxiv_authors and set(zotero_authors) != set(arxiv_authors):
            differences["authors"] = [
                {"source": "Zotero", "value": ", ".join(zotero_authors)},
                {"source": "arXiv", "value": ", ".join(arxiv_authors)}
            ]

        doi_zotero = (zotero.get("doi") or "").strip()
        doi_arxiv = (arxiv.get("doi") or "").strip()
        if doi_zotero and doi_arxiv and doi_zotero != doi_arxiv:
            differences["doi"] = [
                {"source": "Zotero", "value": doi_zotero},
                {"source": "arXiv", "value": doi_arxiv}
            ]

        return differences

    def _normalize_abstract(self, abstract: str) -> str:
        """Normalize abstract for comparison"""
        if not abstract:
            return ""
        normalized = re.sub(r'\s+', ' ', abstract.strip())
        return normalized

    def _normalize_date(self, date: str) -> str:
        """Normalize date for comparison"""
        if not date:
            return ""
        date = date.strip()
        if len(date) >= 10:
            return date[:10]
        return date

    def _extract_last_names(self, creators: List[Dict]) -> List[str]:
        """Extract last names from Zotero creators"""
        last_names = []
        for creator in creators:
            if creator.get("creatorType") in ["author", "coauthor"]:
                last_name = creator.get("lastName", "").strip()
                if last_name:
                    last_names.append(last_name)
        return last_names

    def validate_and_update_item(self, item_key: str, apply_updates: bool = False) -> Dict:
        """
        Validate a Zotero item against arXiv and optionally update it.

        Args:
            item_key: The Zotero item key
            apply_updates: If True, update Zotero with arXiv data

        Returns:
            Dict containing validation results and any updates applied
        """
        validation = self.validate_item_with_arxiv(item_key)

        if not validation.get("success"):
            return validation

        differences = validation.get("differences", {})

        if not differences:
            return {
                "success": True,
                "message": "Item metadata matches arXiv perfectly",
                "item_key": item_key,
                "is_match": True
            }

        if apply_updates:
            updates = {}
            if "title" in differences:
                updates["title"] = validation["arxiv_metadata"].get("title", "")

            if "abstract" in differences:
                updates["abstractNote"] = validation["arxiv_metadata"].get("abstract", "")

            if "date" in differences:
                updates["date"] = validation["arxiv_metadata"].get("date", "")

            if updates:
                update_result = self.update_item(item_key, updates)
                validation["update_result"] = update_result
                validation["updates_applied"] = updates
                validation["message"] = f"Applied {len(updates)} updates from arXiv"

        return validation

    def _get_item_attachments(self, item_id: int) -> List[Dict]:
        """Get attachments for an item from the database"""
        attachments = []
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return attachments

        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("""
                SELECT a.itemID, a.path, a.filename, a.contentType, a.storagePath,
                       i.key, i.itemType
                FROM attachments a
                JOIN items i ON a.itemID = i.itemID
                WHERE a.parentItemID = ?
            """, (item_id,))

            for row in cursor:
                attachments.append({
                    "attachmentItemID": row["itemID"],
                    "key": row["key"],
                    "filename": row["filename"],
                    "contentType": row["contentType"],
                    "path": row["path"],
                    "storagePath": row["storagePath"]
                })

            conn.close()
        except Exception as e:
            logger.error(f"Failed to get attachments: {e}")

        return attachments

    def _get_item_notes(self, item_id: int) -> List[Dict]:
        """Get notes for an item from the database"""
        notes = []
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return notes

        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("""
                SELECT n.itemID, n.note, i.key
                FROM notes n
                JOIN items i ON n.itemID = i.itemID
                WHERE n.parentItemID = ?
            """, (item_id,))

            for row in cursor:
                notes.append({
                    "noteItemID": row["itemID"],
                    "key": row["key"],
                    "note": row["note"]
                })

            conn.close()
        except Exception as e:
            logger.error(f"Failed to get notes: {e}")

        return notes

    def _get_item_tags(self, item_id: int) -> List[Dict]:
        """Get tags for an item from the database"""
        tags = []
        db_path = self._get_zotero_db_path()
        if not db_path or not db_path.exists():
            return tags

        try:
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("""
                SELECT t.tagID, t.name, it.type
                FROM tags t
                JOIN itemTags it ON t.tagID = it.tagID
                WHERE it.itemID = ?
            """, (item_id,))

            for row in cursor:
                tags.append({
                    "tagID": row["tagID"],
                    "name": row["name"],
                    "type": row["type"]
                })

            conn.close()
        except Exception as e:
            logger.error(f"Failed to get tags: {e}")

        return tags

    def _find_attachment_storage_path(self, attachment_key: str) -> Optional[Path]:
        """Find the actual storage path for an attachment"""
        storage_dir = self._zotero_storage_dir
        if not storage_dir or not storage_dir.exists():
            return None

        try:
            for item_folder in storage_dir.iterdir():
                if item_folder.is_dir():
                    attachment_path = item_folder / f"{attachment_key}.pdf"
                    if attachment_path.exists():
                        return attachment_path
                    attachment_path = item_folder / f"{attachment_key}"
                    if attachment_path.exists():
                        return attachment_path
        except Exception as e:
            logger.error(f"Failed to find attachment storage path: {e}")

        return None

    def get_item_pdf_content(self, item_key: str) -> Dict:
        """
        Get the PDF content of an item's attachment for text extraction.

        Args:
            item_key: The Zotero item key

        Returns:
            Dict containing PDF content or path info
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            db_path = self._get_zotero_db_path()
            if not db_path or not db_path.exists():
                return {"success": False, "error": "Zotero database not found"}

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            cursor.execute("SELECT itemID FROM items WHERE key = ? AND libraryID = 1", (item_key,))
            row = cursor.fetchone()
            if not row:
                conn.close()
                return {"success": False, "error": "Item not found"}

            item_id = row[0]

            cursor.execute("""
                SELECT a.itemID, i.key, a.path, a.filename
                FROM attachments a
                JOIN items i ON a.itemID = i.itemID
                WHERE a.parentItemID = ? AND a.contentType = 'application/pdf'
                LIMIT 1
            """, (item_id,))

            attachment_row = cursor.fetchone()
            conn.close()

            if not attachment_row:
                return {"success": False, "error": "No PDF attachment found", "item_key": item_key}

            attachment_key = attachment_row[1]
            storage_path = self._find_attachment_storage_path(attachment_key)

            if storage_path and storage_path.exists():
                try:
                    from pypdf import PdfReader
                    reader = PdfReader(str(storage_path))
                    text_content = []
                    for page in reader.pages:
                        text = page.extract_text()
                        if text:
                            text_content.append(text)
                    full_text = "\n\n".join(text_content)

                    return {
                        "success": True,
                        "item_key": item_key,
                        "attachment_key": attachment_key,
                        "pdf_path": str(storage_path),
                        "text": full_text,
                        "page_count": len(reader.pages),
                        "character_count": len(full_text)
                    }
                except Exception as e:
                    return {"success": False, "error": f"PDF read failed: {e}", "item_key": item_key}
            else:
                return {
                    "success": False,
                    "error": "PDF file not found in storage",
                    "item_key": item_key,
                    "attachment_key": attachment_key,
                    "suggestion": "The PDF may be stored remotely or not yet synced"
                }

        except Exception as e:
            logger.error(f"Failed to get item PDF content: {e}")
            return {"success": False, "error": str(e)}

    def get_item_full_data(self, item_key: str, include_attachments: bool = True) -> Dict:
        """
        Get full item data including attachments, notes, and tags.

        Args:
            item_key: The Zotero item key
            include_attachments: Whether to include attachment metadata

        Returns:
            Dict containing complete item data
        """
        try:
            if not self.is_running():
                return {"success": False, "error": "Zotero is not running"}

            item = self._get_item_from_database(item_key)
            if not item:
                return {"success": False, "error": "Item not found"}

            item_id = item.get("itemID")

            if include_attachments:
                item["attachments"] = self._get_item_attachments(item_id)

            item["notes"] = self._get_item_notes(item_id)

            raw_tags = self._get_item_tags(item_id)
            item["tags"] = [t["name"] for t in raw_tags]
            item["tags_detail"] = raw_tags

            return {"success": True, "item": item}

        except Exception as e:
            logger.error(f"Failed to get item full data: {e}")
            return {"success": False, "error": str(e)}


def test_zotero_connection():
    """Test Zotero connection"""
    print("Testing Zotero connection...")

    connector = ZoteroConnector()

    if connector.is_running():
        version = connector.get_version()
        if version:
            print(f"Zotero connection successful, version: {version}")

            collections = connector.get_collections()
            print(f"Found {len(collections)} collections")
        else:
            print("Zotero connection successful, but could not get version info")
    else:
        print("Zotero is not running or connection failed")


if __name__ == "__main__":
    test_zotero_connection()